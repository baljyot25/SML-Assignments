{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "import math\n",
    "from typing import Counter\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "data = np.load(\"C:\\\\Users\\\\baljyot\\\\Downloads\\\\mnist.npz\")\n",
    "x_train=data['x_train']\n",
    "y_train=data['y_train']\n",
    "x_test=data['x_test']\n",
    "y_test=data['y_test']\n",
    "\n",
    "indices_0 = np.where(y_train == 0)[0]\n",
    "indices_1 = np.where(y_train == 1)[0]\n",
    "\n",
    "val_indices_0 = indices_0[-1000:]\n",
    "val_indices_1 = indices_1[-1000:]\n",
    "\n",
    "train_indices_0 = indices_0[:-1000]\n",
    "train_indices_1 = indices_1[0:-1000]\n",
    "\n",
    "total_1=len(train_indices_1)\n",
    "total_n1=len(train_indices_0)\n",
    "\n",
    "train_indices = np.concatenate([train_indices_0, train_indices_1])\n",
    "\n",
    "val_indices = np.concatenate([val_indices_0, val_indices_1])\n",
    "\n",
    "np.random.shuffle(train_indices)\n",
    "\n",
    "x_val = np.array(x_train[val_indices])\n",
    "y_val = np.array(y_train[val_indices])\n",
    "x_train = np.array(x_train[train_indices])\n",
    "y_train = np.array(y_train[train_indices])\n",
    "\n",
    "indices = np.where((y_test == 0) | (y_test == 1))[0]\n",
    "x_test =np.array(x_test[indices])\n",
    "y_test = np.array(y_test[indices])\n",
    "\n",
    "\n",
    "flattened=[]\n",
    "for i in range(len(x_train)):\n",
    "    flattened.append(x_train[i].flatten())\n",
    "x_train=np.array(flattened)\n",
    "flattened=[]\n",
    "for i in range(len(x_val)):\n",
    "    flattened.append(x_val[i].flatten())\n",
    "x_val=np.array(flattened)\n",
    "flattened=[]\n",
    "for i in range(len(x_test)):\n",
    "    flattened.append(x_test[i].flatten())\n",
    "x_test=np.array(flattened)\n",
    "\n",
    "\n",
    "x_train=np.transpose(x_train)\n",
    "sums=[]\n",
    "centralisedMean=np.mean(x_train,axis=1)\n",
    "centralisedData=[]\n",
    "for i in range(x_train.shape[0]):\n",
    "    l=[]\n",
    "    for j in range(x_train.shape[1]):\n",
    "        l.append(x_train[i][j]- centralisedMean[i])\n",
    "    centralisedData.append(l)\n",
    "centralisedMean=np.array(np.mean(centralisedData,axis=1))\n",
    "\n",
    "S=np.matmul( centralisedData ,np.transpose(centralisedData))/x_train.shape[1]\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(S)\n",
    "eigenvalues = eigenvalues[::-1]\n",
    "U = np.flip(eigenvectors, axis=1)\n",
    "\n",
    "nUp=U[:,:5]\n",
    "\n",
    "x_proj=np.matmul(nUp.T,x_train-x_train.mean(axis=1)[:,np.newaxis])\n",
    "\n",
    "x_test=np.transpose(x_test)\n",
    "x_proj_test=np.matmul(nUp.T,x_test-x_train.mean(axis=1)[:,np.newaxis])\n",
    "\n",
    "x_val=np.transpose(x_val)\n",
    "x_val_proj=np.matmul(nUp.T,x_val-x_train.mean(axis=1)[:,np.newaxis])\n",
    "\n",
    "\n",
    "x_val=x_val_proj\n",
    "x_test=x_proj_test\n",
    "x_train=x_proj\n",
    "\n",
    "\n",
    "y_train = np.array(y_train).astype(np.int8)\n",
    "y_val = np.array(y_val).astype(np.int8)\n",
    "y_test = np.array(y_test).astype(np.int8)\n",
    "\n",
    "y_train[y_train == 0] = -1\n",
    "y_val[y_val == 0] = -1\n",
    "y_test[y_test == 0] = -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Region:\n",
    "   def __init__(self):\n",
    "      self.one=0\n",
    "      self.n_one=0\n",
    "      self.tot_weight=None\n",
    "      self.misclassified_weight=None\n",
    "      self.category=None\n",
    "\n",
    "   def classify(self):\n",
    "      if (self.one>=self.n_one): self.category=1\n",
    "      else: self.category=-1\n",
    "\n",
    "   def make_region_classify(self,one,n_one):\n",
    "      self.one=one\n",
    "      self.n_one=n_one\n",
    "      self.classify()\n",
    "\n",
    "\n",
    "   def calc_loss(self, weights, x_train, y_train):\n",
    "      self.misclassified_weight=0\n",
    "      self.tot_weight=0\n",
    "      for i in range(len(y_train)):\n",
    "         self.tot_weight+=weights[i]\n",
    "         if (y_train[i]!=self.category):\n",
    "            self.misclassified_weight+=weights[i]\n",
    "      # print(self.misclassified_weight)\n",
    "\n",
    "   def remove(self, to_remove, index, y_train, weights):\n",
    "      remove_class = y_train[index]\n",
    "      self.tot_weight -= weights[index]\n",
    "      if remove_class != self.category:\n",
    "         self.misclassified_weight -= weights[index]\n",
    "      if remove_class == 1:\n",
    "         self.one -= 1\n",
    "      else:\n",
    "         self.n_one -= 1\n",
    "      prev_cat = self.category\n",
    "      self.classify()\n",
    "      if prev_cat != self.category:\n",
    "         # print(\"category changed in removing\")\n",
    "         self.misclassified_weight = self.tot_weight - self.misclassified_weight\n",
    "   \n",
    "   def add(self, to_add, index, y_train , weights):\n",
    "      add_class=y_train[index]\n",
    "      if (add_class==1):self.one+=1\n",
    "      else: self.n_one+=1\n",
    "      prev_cat=self.category\n",
    "      self.classify()\n",
    "      if (prev_cat!=self.category):\n",
    "         # print(\"category changed in adding\")\n",
    "         self.misclassified_weight = self.tot_weight - self.misclassified_weight\n",
    "      self.tot_weight+=weights[index]\n",
    "      if (add_class!=self.category):\n",
    "          self.misclassified_weight =  self.misclassified_weight+weights[index]\n",
    "\n",
    "   def print_info(self):\n",
    "      print(\"one \",self.one,end=\"  \")\n",
    "      print(\"n_one \",self.n_one,end=\"  \")\n",
    "      print(\"class \", self.category,end=\"  \")\n",
    "\n",
    "      print(\"misclassified \",self.misclassified_weight,end=\"  \")\n",
    "      print(\"tot weight \",self.tot_weight)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stump:\n",
    "    def __init__(self,left,right,cut,dim,loss,alpha):\n",
    "        self.Left=left \n",
    "        self.Right=right \n",
    "        self.cut=cut \n",
    "        self.dim=dim \n",
    "        self.loss=loss \n",
    "        self.alpha=alpha \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tree, data):\n",
    "    if (data[tree.dim]<=tree.cut):\n",
    "            return tree.Left.category\n",
    "    else:\n",
    "        return tree.Right.category\n",
    "\n",
    "    \n",
    "\n",
    "def prediction(stumps , data):\n",
    "    ans=0\n",
    "    for i in range(len(stumps)):\n",
    "        ans+=stumps[i].alpha* predict(stumps[i], data)\n",
    "\n",
    "    if (ans<0):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stump 1 cut 196.10629969876373  best dim  0 alpha  5.399010758105682\n",
      "Total Accuracy 99.55 %\n",
      "stump 2 cut -305.0037444588237  best dim  0 alpha  1.0858901576149438\n",
      "Total Accuracy 99.55 %\n",
      "stump 3 cut 637.8272710940987  best dim  0 alpha  1.4888908078905678\n",
      "Total Accuracy 99.55 %\n",
      "stump 4 cut 337.3657512224445  best dim  2 alpha  1.1000026806204484\n",
      "Total Accuracy 99.55 %\n",
      "stump 5 cut 475.24469266725686  best dim  1 alpha  1.0319797451790491\n",
      "Total Accuracy 99.55 %\n",
      "stump 6 cut -305.0037444588237  best dim  0 alpha  1.167530341376095\n",
      "Total Accuracy 99.55 %\n",
      "stump 7 cut -148.37821234691356  best dim  3 alpha  0.978219958879273\n",
      "Total Accuracy 99.55 %\n",
      "stump 8 cut 252.63889625082882  best dim  1 alpha  0.6581598641900991\n",
      "Total Accuracy 99.55 %\n",
      "stump 9 cut -305.0037444588237  best dim  0 alpha  0.7612241832593208\n",
      "Total Accuracy 99.55 %\n",
      "stump 10 cut 637.8272710940987  best dim  0 alpha  0.6012447802517221\n",
      "Total Accuracy 99.55 %\n",
      "stump 11 cut 470.2325962203453  best dim  2 alpha  0.46473549214585846\n",
      "Total Accuracy 99.55 %\n",
      "stump 12 cut 475.24469266725686  best dim  1 alpha  0.4755730681208094\n",
      "Total Accuracy 99.55 %\n",
      "stump 13 cut -253.44094480868011  best dim  0 alpha  0.550298392035732\n",
      "Total Accuracy 99.55 %\n",
      "stump 14 cut 637.8272710940987  best dim  0 alpha  0.4394175848814071\n",
      "Total Accuracy 99.55 %\n",
      "stump 15 cut 192.98741523654476  best dim  2 alpha  0.39832878436089175\n",
      "Total Accuracy 99.55 %\n",
      "stump 16 cut -305.0037444588237  best dim  0 alpha  0.32851695317715707\n",
      "Total Accuracy 99.55 %\n",
      "stump 17 cut 475.24469266725686  best dim  1 alpha  0.4924024097655555\n",
      "Total Accuracy 99.55 %\n",
      "stump 18 cut -488.4669047036617  best dim  2 alpha  0.3842020079784755\n",
      "Total Accuracy 99.55 %\n",
      "stump 19 cut 637.8272710940987  best dim  0 alpha  0.34468402506369383\n",
      "Total Accuracy 99.55 %\n",
      "stump 20 cut 87.56535421834403  best dim  3 alpha  0.32727953243590896\n",
      "Total Accuracy 99.55 %\n",
      "stump 21 cut 204.4866350119695  best dim  1 alpha  0.3203297388029244\n",
      "Total Accuracy 99.55 %\n",
      "stump 22 cut 247.385667179579  best dim  2 alpha  0.27995244576147343\n",
      "Total Accuracy 99.55 %\n",
      "stump 23 cut 637.8272710940987  best dim  0 alpha  0.23008583626238172\n",
      "Total Accuracy 99.55 %\n",
      "stump 24 cut -335.5836685494529  best dim  2 alpha  0.3499917791352439\n",
      "Total Accuracy 99.55 %\n",
      "stump 25 cut 544.0757777418571  best dim  1 alpha  0.30918085764185765\n",
      "Total Accuracy 99.55 %\n",
      "stump 26 cut -305.0037444588237  best dim  0 alpha  0.287711732733981\n",
      "Total Accuracy 99.55 %\n",
      "stump 27 cut 637.8272710940987  best dim  0 alpha  0.28823002872865183\n",
      "Total Accuracy 99.55 %\n",
      "stump 28 cut -549.0597297787168  best dim  2 alpha  0.25834865698410536\n",
      "Total Accuracy 99.55 %\n",
      "stump 29 cut 164.45056342905212  best dim  1 alpha  0.24356684049092062\n",
      "Total Accuracy 99.55 %\n",
      "stump 30 cut 248.29959594853054  best dim  2 alpha  0.21534822352236302\n",
      "Total Accuracy 99.55 %\n",
      "stump 31 cut 637.8272710940987  best dim  0 alpha  0.19325820615569453\n",
      "Total Accuracy 99.55 %\n",
      "stump 32 cut -507.4472277454546  best dim  2 alpha  0.25695040986161827\n",
      "Total Accuracy 99.55 %\n",
      "stump 33 cut 544.0757777418571  best dim  1 alpha  0.22817195184555514\n",
      "Total Accuracy 99.55 %\n",
      "stump 34 cut -305.0037444588237  best dim  0 alpha  0.21175673621978275\n",
      "Total Accuracy 99.55 %\n",
      "stump 35 cut 637.8272710940987  best dim  0 alpha  0.2243053594565559\n",
      "Total Accuracy 99.55 %\n",
      "stump 36 cut 87.56535421834403  best dim  3 alpha  0.2126942749992299\n",
      "Total Accuracy 99.55 %\n",
      "stump 37 cut 637.8272710940987  best dim  0 alpha  0.1888458882539095\n",
      "Total Accuracy 99.55 %\n",
      "stump 38 cut -536.4718965551774  best dim  2 alpha  0.18854243272840762\n",
      "Total Accuracy 99.55 %\n",
      "stump 39 cut 475.24469266725686  best dim  1 alpha  0.19455330203098706\n",
      "Total Accuracy 99.55 %\n",
      "stump 40 cut -305.0037444588237  best dim  0 alpha  0.1754630607979377\n",
      "Total Accuracy 99.55 %\n",
      "stump 41 cut 637.8272710940987  best dim  0 alpha  0.17234601411117628\n",
      "Total Accuracy 99.55 %\n",
      "stump 42 cut 248.29959594853054  best dim  2 alpha  0.1759607358916021\n",
      "Total Accuracy 99.55 %\n",
      "stump 43 cut 34.99027640528982  best dim  4 alpha  0.18055933020067716\n",
      "Total Accuracy 99.55 %\n",
      "stump 44 cut -507.4472277454546  best dim  2 alpha  0.13454621831677943\n",
      "Total Accuracy 99.55 %\n",
      "stump 45 cut 237.3477507380754  best dim  1 alpha  0.1827051766398237\n",
      "Total Accuracy 99.55 %\n",
      "stump 46 cut -253.44094480868011  best dim  0 alpha  0.15400595829350122\n",
      "Total Accuracy 99.55 %\n",
      "stump 47 cut 637.8272710940987  best dim  0 alpha  0.1659811370874651\n",
      "Total Accuracy 99.55 %\n",
      "stump 48 cut 247.385667179579  best dim  2 alpha  0.1654659757465998\n",
      "Total Accuracy 99.55 %\n",
      "stump 49 cut 475.24469266725686  best dim  1 alpha  0.11798585318522667\n",
      "Total Accuracy 99.55 %\n",
      "stump 50 cut -488.4669047036617  best dim  2 alpha  0.1727858230633951\n",
      "Total Accuracy 99.55 %\n",
      "stump 51 cut 637.8272710940987  best dim  0 alpha  0.16445002774497336\n",
      "Total Accuracy 99.55 %\n",
      "stump 52 cut -575.4141170983552  best dim  2 alpha  0.15387213671187977\n",
      "Total Accuracy 99.55 %\n",
      "stump 53 cut 455.60333667693027  best dim  1 alpha  0.15632927097853844\n",
      "Total Accuracy 99.55 %\n",
      "stump 54 cut -575.4141170983552  best dim  2 alpha  0.14240146418142607\n",
      "Total Accuracy 99.55 %\n",
      "stump 55 cut 637.8272710940987  best dim  0 alpha  0.14269803106355708\n",
      "Total Accuracy 99.55 %\n",
      "stump 56 cut 87.56535421834403  best dim  3 alpha  0.13849006616613493\n",
      "Total Accuracy 99.55 %\n",
      "stump 57 cut 164.45056342905212  best dim  1 alpha  0.12897289875218124\n",
      "Total Accuracy 99.55 %\n",
      "stump 58 cut -253.44094480868011  best dim  0 alpha  0.12000975886999188\n",
      "Total Accuracy 99.55 %\n",
      "stump 59 cut 637.8272710940987  best dim  0 alpha  0.13988540057518206\n",
      "Total Accuracy 99.55 %\n",
      "stump 60 cut -482.77958584154726  best dim  2 alpha  0.13204796271371672\n",
      "Total Accuracy 99.55 %\n",
      "stump 61 cut 637.8272710940987  best dim  0 alpha  0.12299622917101995\n",
      "Total Accuracy 99.55 %\n",
      "stump 62 cut 248.29959594853054  best dim  2 alpha  0.12509500237907997\n",
      "Total Accuracy 99.65 %\n",
      "stump 63 cut 36.038907174263414  best dim  4 alpha  0.10287676290374187\n",
      "Total Accuracy 99.55 %\n",
      "stump 64 cut -305.0037444588237  best dim  0 alpha  0.0853375591806109\n",
      "Total Accuracy 99.55 %\n",
      "stump 65 cut 455.60333667693027  best dim  1 alpha  0.1316535079772267\n",
      "Total Accuracy 99.65 %\n",
      "stump 66 cut -536.6950650116725  best dim  2 alpha  0.12133094362949769\n",
      "Total Accuracy 99.55 %\n",
      "stump 67 cut 637.8272710940987  best dim  0 alpha  0.11934398152014672\n",
      "Total Accuracy 99.65 %\n",
      "stump 68 cut -253.44094480868011  best dim  0 alpha  0.1165892111829108\n",
      "Total Accuracy 99.55 %\n",
      "stump 69 cut 475.24469266725686  best dim  1 alpha  0.11397889632430798\n",
      "Total Accuracy 99.65 %\n",
      "stump 70 cut 247.385667179579  best dim  2 alpha  0.1106755229243904\n",
      "Total Accuracy 99.65 %\n",
      "stump 71 cut 637.8272710940987  best dim  0 alpha  0.0815917958441969\n",
      "Total Accuracy 99.65 %\n",
      "stump 72 cut -482.77958584154726  best dim  2 alpha  0.1165351131423756\n",
      "Total Accuracy 99.65 %\n",
      "stump 73 cut 164.45056342905212  best dim  1 alpha  0.11276742862183406\n",
      "Total Accuracy 99.65 %\n",
      "stump 74 cut -605.4558957202044  best dim  2 alpha  0.09339944060545843\n",
      "Total Accuracy 99.65 %\n",
      "stump 75 cut 637.8272710940987  best dim  0 alpha  0.11239694914241104\n",
      "Total Accuracy 99.65 %\n",
      "stump 76 cut -585.3498620258733  best dim  3 alpha  0.1137030029462126\n",
      "Total Accuracy 99.65 %\n",
      "stump 77 cut 743.8959056741593  best dim  1 alpha  0.1079027631532989\n",
      "Total Accuracy 99.65 %\n",
      "stump 78 cut 87.56535421834403  best dim  3 alpha  0.10854278198536232\n",
      "Total Accuracy 99.65 %\n",
      "stump 79 cut 637.8272710940987  best dim  0 alpha  0.10200373180557283\n",
      "Total Accuracy 99.65 %\n",
      "stump 80 cut -585.3498620258733  best dim  3 alpha  0.10310642544647476\n",
      "Total Accuracy 99.65 %\n",
      "stump 81 cut 637.8272710940987  best dim  0 alpha  0.09804750201387256\n",
      "Total Accuracy 99.65 %\n",
      "stump 82 cut -536.6950650116725  best dim  2 alpha  0.10194732626018217\n",
      "Total Accuracy 99.65 %\n",
      "stump 83 cut 455.60333667693027  best dim  1 alpha  0.10037482569877872\n",
      "Total Accuracy 99.65 %\n",
      "stump 84 cut -253.44094480868011  best dim  0 alpha  0.09483788504905238\n",
      "Total Accuracy 99.65 %\n",
      "stump 85 cut 637.8272710940987  best dim  0 alpha  0.09576316302110781\n",
      "Total Accuracy 99.65 %\n",
      "stump 86 cut -536.6950650116725  best dim  2 alpha  0.09529418619410246\n",
      "Total Accuracy 99.65 %\n",
      "stump 87 cut 484.5911056533217  best dim  1 alpha  0.09137561860084449\n",
      "Total Accuracy 99.65 %\n",
      "stump 88 cut 247.385667179579  best dim  2 alpha  0.0863235747574203\n",
      "Total Accuracy 99.65 %\n",
      "stump 89 cut 35.516410969571155  best dim  4 alpha  0.0728912453996748\n",
      "Total Accuracy 99.65 %\n",
      "stump 90 cut -305.0037444588237  best dim  0 alpha  0.06209334933584483\n",
      "Total Accuracy 99.65 %\n",
      "stump 91 cut 637.8272710940987  best dim  0 alpha  0.09496301218164432\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 92 cut -575.4141170983552  best dim  2 alpha  0.0920502709360646\n",
      "Total Accuracy 99.65 %\n",
      "stump 93 cut 164.45056342905212  best dim  1 alpha  0.08976011997710433\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 94 cut -585.3498620258733  best dim  3 alpha  0.07904573523709663\n",
      "Total Accuracy 99.65 %\n",
      "stump 95 cut 637.8272710940987  best dim  0 alpha  0.09320833825101645\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 96 cut -536.4718965551774  best dim  2 alpha  0.09102083739880132\n",
      "Total Accuracy 99.65 %\n",
      "stump 97 cut 743.8959056741593  best dim  1 alpha  0.08746159792561102\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 98 cut -585.3498620258733  best dim  3 alpha  0.08872400243684132\n",
      "Total Accuracy 99.65 %\n",
      "stump 99 cut 637.8272710940987  best dim  0 alpha  0.08590833729436147\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 100 cut 248.29959594853054  best dim  2 alpha  0.08911888413090376\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 101 cut 743.8959056741593  best dim  1 alpha  0.06360613593369113\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 102 cut 87.56535421834403  best dim  3 alpha  0.08725663747568342\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 103 cut 164.45056342905212  best dim  1 alpha  0.08248155108210253\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 104 cut -253.44094480868011  best dim  0 alpha  0.07521236499400925\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 105 cut 637.8272710940987  best dim  0 alpha  0.08648360261702064\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 106 cut -536.4718965551774  best dim  2 alpha  0.0852867592662546\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 107 cut 637.8272710940987  best dim  0 alpha  0.08179664559432306\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 108 cut -585.3498620258733  best dim  3 alpha  0.08476615814198424\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 109 cut 743.8959056741593  best dim  1 alpha  0.08178601557096549\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 110 cut -305.0037444588237  best dim  0 alpha  0.07907889653537288\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 111 cut 455.60333667693027  best dim  1 alpha  0.07856636510774576\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 112 cut -536.6950650116725  best dim  2 alpha  0.07569549882581635\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 113 cut 637.8272710940987  best dim  0 alpha  0.07795516815197641\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 114 cut -585.3498620258733  best dim  3 alpha  0.07873273653200819\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 115 cut 637.8272710940987  best dim  0 alpha  0.07574928015102939\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 116 cut 87.56535421834403  best dim  3 alpha  0.0754362443532664\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 117 cut 637.8272710940987  best dim  0 alpha  0.07029554072084068\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 118 cut -536.4718965551774  best dim  2 alpha  0.07304409984620588\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 119 cut 164.45056342905212  best dim  1 alpha  0.0728830513859568\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 120 cut -585.3498620258733  best dim  3 alpha  0.0646892797555443\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 121 cut 637.8272710940987  best dim  0 alpha  0.07419002737407128\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 122 cut -305.0037444588237  best dim  0 alpha  0.0724643535234454\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 123 cut 637.8272710940987  best dim  0 alpha  0.06992958241188323\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 124 cut 248.29959594853054  best dim  2 alpha  0.07111714297862769\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 125 cut 455.60333667693027  best dim  1 alpha  0.052939773496557554\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 126 cut -482.77958584154726  best dim  2 alpha  0.0716318960467007\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 127 cut 743.8959056741593  best dim  1 alpha  0.07108701591039199\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 128 cut -585.3498620258733  best dim  3 alpha  0.07098684306809178\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 129 cut 637.8272710940987  best dim  0 alpha  0.06907565164049242\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 130 cut -535.7165946405441  best dim  2 alpha  0.06888846994005204\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 131 cut 455.60333667693027  best dim  1 alpha  0.06721893645978688\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 132 cut -305.0037444588237  best dim  0 alpha  0.06382687509637157\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 133 cut 637.8272710940987  best dim  0 alpha  0.06638961470532034\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 134 cut -585.3498620258733  best dim  3 alpha  0.06668963737033101\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 135 cut 637.8272710940987  best dim  0 alpha  0.06453689438785347\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 136 cut -536.6950650116725  best dim  2 alpha  0.06590489705187094\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 137 cut 637.8272710940987  best dim  0 alpha  0.06380173680335394\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 138 cut 85.69979090642707  best dim  3 alpha  0.06312908317338425\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 139 cut 164.45056342905212  best dim  1 alpha  0.06424402416829855\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 140 cut -585.3498620258733  best dim  3 alpha  0.05859400458628702\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 141 cut 637.8272710940987  best dim  0 alpha  0.06340557998256997\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 142 cut -535.7165946405441  best dim  2 alpha  0.06420491908862304\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 143 cut 743.8959056741593  best dim  1 alpha  0.06225499270529143\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 144 cut -585.3498620258733  best dim  3 alpha  0.06124095349110053\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 145 cut 637.8272710940987  best dim  0 alpha  0.059942960832925586\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 146 cut 187.96149770270947  best dim  2 alpha  0.06159656869139165\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 147 cut 35.516410969571155  best dim  4 alpha  0.056692787786085566\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 148 cut -305.0037444588237  best dim  0 alpha  0.06554418462702238\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 149 cut 743.8959056741593  best dim  1 alpha  0.060942460454016835\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 150 cut -536.4718965551774  best dim  2 alpha  0.05997198684420509\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 151 cut 455.60333667693027  best dim  1 alpha  0.059275834617595426\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 152 cut -253.44094480868011  best dim  0 alpha  0.0573593384413389\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 153 cut 637.8272710940987  best dim  0 alpha  0.058060620253262744\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 154 cut -536.6950650116725  best dim  2 alpha  0.05802822722830332\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 155 cut 637.8272710940987  best dim  0 alpha  0.05639162791430053\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 156 cut -585.3498620258733  best dim  3 alpha  0.05786217328092809\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 157 cut 455.60333667693027  best dim  1 alpha  0.05636053433308354\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 158 cut 248.29959594853054  best dim  2 alpha  0.05500701382960266\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 159 cut 164.45056342905212  best dim  1 alpha  0.04560159721538709\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 160 cut -535.7165946405441  best dim  2 alpha  0.05415083626088404\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 161 cut 637.8272710940987  best dim  0 alpha  0.058885233469175666\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 162 cut 85.69979090642707  best dim  3 alpha  0.058122609387813\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 163 cut 637.8272710940987  best dim  0 alpha  0.05431052086850419\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 164 cut -585.3498620258733  best dim  3 alpha  0.05698131553389116\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 165 cut 637.8272710940987  best dim  0 alpha  0.05540244805143934\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 166 cut -535.7165946405441  best dim  2 alpha  0.05621114025622503\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 167 cut 637.8272710940987  best dim  0 alpha  0.05467409991908138\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 168 cut -305.0037444588237  best dim  0 alpha  0.05454702947073429\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 169 cut 743.8959056741593  best dim  1 alpha  0.053481205402016875\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 170 cut -585.3498620258733  best dim  3 alpha  0.05361000722337573\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 171 cut 164.45056342905212  best dim  1 alpha  0.05403889324220643\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 172 cut -535.7165946405441  best dim  2 alpha  0.0471781995701925\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 173 cut 637.8272710940987  best dim  0 alpha  0.053508419409938654\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 174 cut -253.44094480868011  best dim  0 alpha  0.05306575617584168\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 175 cut 637.8272710940987  best dim  0 alpha  0.05089471571326969\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 176 cut -585.3498620258733  best dim  3 alpha  0.05202903069168165\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 177 cut 743.8959056741593  best dim  1 alpha  0.05071539462537625\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 178 cut 85.69979090642707  best dim  3 alpha  0.05106523416053134\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 179 cut 637.8272710940987  best dim  0 alpha  0.04816904991257064\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 180 cut -535.7165946405441  best dim  2 alpha  0.050262807788814506\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 181 cut 455.60333667693027  best dim  1 alpha  0.05027855672394935\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 182 cut -585.3498620258733  best dim  3 alpha  0.048232538430702104\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 183 cut 637.8272710940987  best dim  0 alpha  0.04889937140693608\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 184 cut -535.7165946405441  best dim  2 alpha  0.049942107689322564\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 185 cut 637.8272710940987  best dim  0 alpha  0.04872514277169167\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 186 cut -305.0037444588237  best dim  0 alpha  0.048455301280368956\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 187 cut 637.8272710940987  best dim  0 alpha  0.04730889883441298\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 188 cut -585.3498620258733  best dim  3 alpha  0.04730189124000556\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 189 cut 164.45056342905212  best dim  1 alpha  0.047403469795869\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 190 cut 248.29959594853054  best dim  2 alpha  0.04631110997253962\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 191 cut 35.516410969571155  best dim  4 alpha  0.04959352474806983\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 192 cut 248.29959594853054  best dim  2 alpha  0.034012022836666544\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 193 cut 743.8959056741593  best dim  1 alpha  0.04176234655273702\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 194 cut -535.7165946405441  best dim  2 alpha  0.04914320792334861\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 195 cut 637.8272710940987  best dim  0 alpha  0.048103025778729304\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 196 cut -305.0037444588237  best dim  0 alpha  0.048048611108435876\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 197 cut 455.60333667693027  best dim  1 alpha  0.04760226297437816\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 198 cut -536.6950650116725  best dim  2 alpha  0.04581878828724615\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 199 cut 637.8272710940987  best dim  0 alpha  0.04689284789777494\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 200 cut -585.3498620258733  best dim  3 alpha  0.04708239955846087\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 201 cut 637.8272710940987  best dim  0 alpha  0.04599932481134378\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 202 cut 268.93597871110217  best dim  3 alpha  0.04683548499067261\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 203 cut 743.8959056741593  best dim  1 alpha  0.04576888348742019\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 204 cut -305.0037444588237  best dim  0 alpha  0.04557827761685454\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 205 cut 637.8272710940987  best dim  0 alpha  0.044835901886326426\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 206 cut -536.6950650116725  best dim  2 alpha  0.0448292291531125\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 207 cut 455.60333667693027  best dim  1 alpha  0.04466114683240822\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 208 cut -585.3498620258733  best dim  3 alpha  0.043383961571123224\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 209 cut 637.8272710940987  best dim  0 alpha  0.044134496844644\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 210 cut -535.7165946405441  best dim  2 alpha  0.04458741563645358\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 211 cut 637.8272710940987  best dim  0 alpha  0.04361491977472546\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 212 cut 268.93597871110217  best dim  3 alpha  0.044316028071077956\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 213 cut 637.8272710940987  best dim  0 alpha  0.04335520892046605\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 214 cut -585.3498620258733  best dim  3 alpha  0.04299632443363687\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 215 cut 164.45056342905212  best dim  1 alpha  0.043439593532605895\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 216 cut -253.44094480868011  best dim  0 alpha  0.0392297128746118\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 217 cut 637.8272710940987  best dim  0 alpha  0.043426741185175956\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 218 cut -535.7165946405441  best dim  2 alpha  0.04350472863501729\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 219 cut 637.8272710940987  best dim  0 alpha  0.04257840468674725\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 220 cut 268.93597871110217  best dim  3 alpha  0.04296709354591978\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 221 cut 743.8959056741593  best dim  1 alpha  0.04215135534557361\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 222 cut 248.29959594853054  best dim  2 alpha  0.042124365631017346\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 223 cut 256.6540282310872  best dim  1 alpha  0.03160922347520695\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 224 cut -535.7165946405441  best dim  2 alpha  0.03959434579050824\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 225 cut 637.8272710940987  best dim  0 alpha  0.04238789769761347\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 226 cut -585.3498620258733  best dim  3 alpha  0.042817026700736664\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 227 cut 637.8272710940987  best dim  0 alpha  0.04191945916052874\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 228 cut 268.93597871110217  best dim  3 alpha  0.04205805840853265\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 229 cut 743.8959056741593  best dim  1 alpha  0.04124798055300427\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 230 cut -535.7165946405441  best dim  2 alpha  0.04132040962747547\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 231 cut 637.8272710940987  best dim  0 alpha  0.04064162423992093\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 232 cut -585.3498620258733  best dim  3 alpha  0.04106384402160097\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 233 cut 637.8272710940987  best dim  0 alpha  0.040237575497971294\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 234 cut -305.0037444588237  best dim  0 alpha  0.040501240837142585\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 235 cut 455.60333667693027  best dim  1 alpha  0.04084887614878152\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 236 cut -536.6950650116725  best dim  2 alpha  0.03960932754091585\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 237 cut 637.8272710940987  best dim  0 alpha  0.03980390224574131\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 238 cut 268.93597871110217  best dim  3 alpha  0.04006587436836573\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 239 cut 637.8272710940987  best dim  0 alpha  0.03927889944331135\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 240 cut -585.3498620258733  best dim  3 alpha  0.03940628875413409\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 241 cut 743.8959056741593  best dim  1 alpha  0.038700362149847505\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 242 cut -305.0037444588237  best dim  0 alpha  0.0388992084468846\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 243 cut 164.45056342905212  best dim  1 alpha  0.03880443770958971\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 244 cut -535.7165946405441  best dim  2 alpha  0.034448727013316936\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 245 cut 637.8272710940987  best dim  0 alpha  0.03921572758454928\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 246 cut 268.93597871110217  best dim  3 alpha  0.0391806503176633\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 247 cut 637.8272710940987  best dim  0 alpha  0.03842774386130321\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 248 cut -585.3498620258733  best dim  3 alpha  0.038864304259626274\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 249 cut 637.8272710940987  best dim  0 alpha  0.03812339320952834\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 250 cut -535.7165946405441  best dim  2 alpha  0.03836674757439622\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 251 cut 637.8272710940987  best dim  0 alpha  0.03764451191073289\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 252 cut 268.93597871110217  best dim  3 alpha  0.03753392536011741\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 253 cut 743.8959056741593  best dim  1 alpha  0.036912989256406806\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 254 cut -585.3498620258733  best dim  3 alpha  0.037322852295246504\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 255 cut 637.8272710940987  best dim  0 alpha  0.036733106538674135\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 256 cut -535.7165946405441  best dim  2 alpha  0.03694230022810645\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 257 cut 484.5911056533217  best dim  1 alpha  0.036844590110579355\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 258 cut -253.44094480868011  best dim  0 alpha  0.03636293694487804\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 259 cut 637.8272710940987  best dim  0 alpha  0.03637201294410859\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 260 cut -535.7165946405441  best dim  2 alpha  0.03621860350545805\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 261 cut 637.8272710940987  best dim  0 alpha  0.035574308445032794\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 262 cut 268.93597871110217  best dim  3 alpha  0.035904802511738027\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 263 cut 455.60333667693027  best dim  1 alpha  0.035432457850441385\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 264 cut 248.29959594853054  best dim  2 alpha  0.03599714972185019\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 265 cut 35.516410969571155  best dim  4 alpha  0.03486496376826626\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 266 cut 187.96149770270947  best dim  2 alpha  0.02803967677863776\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 267 cut 164.45056342905212  best dim  1 alpha  0.02271948987453045\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 268 cut -253.44094480868011  best dim  0 alpha  0.03497891804292551\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 269 cut 637.8272710940987  best dim  0 alpha  0.03664663972250148\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 270 cut -535.7165946405441  best dim  2 alpha  0.03678158254615421\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 271 cut 637.8272710940987  best dim  0 alpha  0.036117283586588805\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 272 cut -585.3498620258733  best dim  3 alpha  0.03655031663525846\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 273 cut 743.8959056741593  best dim  1 alpha  0.03596671005338179\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 274 cut -305.0037444588237  best dim  0 alpha  0.03573588773608709\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 275 cut 637.8272710940987  best dim  0 alpha  0.03518592540206606\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 276 cut -585.3498620258733  best dim  3 alpha  0.0352000841210961\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 277 cut 743.8959056741593  best dim  1 alpha  0.034609301564418\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 278 cut -536.6950650116725  best dim  2 alpha  0.03497984838027323\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 279 cut 455.60333667693027  best dim  1 alpha  0.03512728462652176\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 280 cut 268.93597871110217  best dim  3 alpha  0.03402687858613186\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 281 cut 637.8272710940987  best dim  0 alpha  0.03445433042988493\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 282 cut -535.7165946405441  best dim  2 alpha  0.03494032488563351\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 283 cut 637.8272710940987  best dim  0 alpha  0.03434033368909269\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 284 cut -305.0037444588237  best dim  0 alpha  0.03425486726066826\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 285 cut 256.6540282310872  best dim  1 alpha  0.033699949343415456\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 286 cut -585.3498620258733  best dim  3 alpha  0.031090813663369354\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 287 cut 637.8272710940987  best dim  0 alpha  0.03423704766032574\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 288 cut -536.6950650116725  best dim  2 alpha  0.03438845931287556\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 289 cut 637.8272710940987  best dim  0 alpha  0.033807115683736275\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 290 cut 268.93597871110217  best dim  3 alpha  0.03424536110548467\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 291 cut 637.8272710940987  best dim  0 alpha  0.03366880550990632\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 292 cut -585.3498620258733  best dim  3 alpha  0.03358653226455654\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 293 cut 637.8272710940987  best dim  0 alpha  0.03303176967958196\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 294 cut -535.7165946405441  best dim  2 alpha  0.03319389125815024\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 295 cut 637.8272710940987  best dim  0 alpha  0.0326519201661432\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 296 cut 268.93597871110217  best dim  3 alpha  0.033005779385592474\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 297 cut 743.8959056741593  best dim  1 alpha  0.03250885072353485\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 298 cut -305.0037444588237  best dim  0 alpha  0.032544470593937946\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 299 cut 544.0757777418571  best dim  1 alpha  0.03215350413168261\n",
      "Total Accuracy 99.69999999999999 %\n",
      "stump 300 cut -585.3498620258733  best dim  3 alpha  0.031650587017938464\n",
      "Total Accuracy 99.69999999999999 %\n"
     ]
    }
   ],
   "source": [
    "values=[]\n",
    "weights=np.array([1.0]*(x_train.shape[1]))\n",
    "num_decesion_tree=300\n",
    "stumps=[]\n",
    "minimumm=1000\n",
    "maximum_accuracy=-1\n",
    "max_num_tree=0\n",
    "for st in range(num_decesion_tree):\n",
    "    total_weight=np.sum(weights)\n",
    "    # print(\"total weight for the \", st ,\"the stump is : \", total_weight)\n",
    "    mini_loss=-1\n",
    "    best_cut=None   \n",
    "    best_left=None\n",
    "    best_right=None\n",
    "    best_dimension=None\n",
    "\n",
    "    for itr in range(5):\n",
    "        #all the values are unique so no need to do np.unique() (maybe beacuse of PCA)\n",
    "        curr=x_train[itr]\n",
    "        sorted_indices = np.argsort(curr)\n",
    "        s_vals =curr[sorted_indices]\n",
    "        s_y_train = y_train[sorted_indices]\n",
    "        # s_vals=np.unique(np.array(sorted(x_train[i])))\n",
    "        cut1=(s_vals[0]+s_vals[1])/2\n",
    "        i_fr_1,i_fr_n1,j_fr_1,j_fr_n1=0,0,0,0\n",
    "        i_fr=np.where(s_vals==s_vals[0])[0]\n",
    "        if (s_y_train[0]==1): i_fr_1=1\n",
    "        else: i_fr_n1=1\n",
    "        Left=Region()\n",
    "        Left.make_region_classify(i_fr_1,i_fr_n1)\n",
    "        \n",
    "        j_fr_1=total_1-i_fr_1\n",
    "        j_fr_n1=total_n1-i_fr_n1\n",
    "        Right=Region()\n",
    "        Right.make_region_classify(j_fr_1, j_fr_n1)\n",
    "\n",
    "        Left.misclassified_weight=0\n",
    "        Left.tot_weight=weights[sorted_indices[0]]\n",
    "        Right.misclassified_weight=0\n",
    "        Right.tot_weight=0\n",
    "        for i in range(1,len(sorted_indices)):\n",
    "            Right.tot_weight+=weights[sorted_indices[i]]\n",
    "            if (y_train[sorted_indices[i]]!=Right.category):\n",
    "                Right.misclassified_weight+=weights[sorted_indices[i]]\n",
    "        new_loss=Left.misclassified_weight+Right.misclassified_weight\n",
    "\n",
    "        if (new_loss<=mini_loss or mini_loss==-1):\n",
    "            mini_loss=new_loss\n",
    "            best_cut=cut1   \n",
    "            best_left=copy.deepcopy(Left)\n",
    "            best_right=copy.deepcopy(Right)\n",
    "            best_dimension=itr\n",
    "    \n",
    "        for j in range(1,len(s_vals)-1):\n",
    "            cut=(s_vals[j]+s_vals[j+1])/2\n",
    "\n",
    "            Right.remove(s_vals[j],sorted_indices[j] , y_train,weights)\n",
    "            Left.add(s_vals[j], sorted_indices[j] , y_train , weights)\n",
    "            new_loss=Left.misclassified_weight+Right.misclassified_weight\n",
    "            if (new_loss<=mini_loss or mini_loss==-1):\n",
    "                mini_loss=new_loss\n",
    "                best_cut=cut\n",
    "                best_left=copy.deepcopy(Left)\n",
    "                best_right=copy.deepcopy(Right)\n",
    "                best_dimension=itr\n",
    "    # print()\n",
    "\n",
    "    loss_ratio=(total_weight-mini_loss)/mini_loss\n",
    "    alpha=np.log((total_weight-mini_loss)/mini_loss)\n",
    "    print(\"stump\",st+1,\"cut\",best_cut, \" best dim \", best_dimension,\"alpha \",alpha)\n",
    "    x_train_T=x_train.T\n",
    "    dim_values = x_train_T[:, best_dimension]\n",
    "\n",
    "    left_indices = np.where(dim_values <= best_cut)[0]\n",
    "\n",
    "    right_indices = np.where(dim_values > best_cut)[0]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    misclass_left_indices = [index for index in left_indices if y_train[index] != best_left.category]\n",
    "    misclass_right_indices = [index for index in right_indices if y_train[index] != best_right.category]\n",
    "\n",
    "\n",
    "    # print(loss_ratio)\n",
    "    for index in misclass_left_indices:\n",
    "        weights[index] =(weights[index]) * loss_ratio\n",
    "        \n",
    "    for index in misclass_right_indices:\n",
    "        weights[index] =(weights[index]) * loss_ratio\n",
    "\n",
    "    tree=Stump(best_left,best_right,best_cut,best_dimension,mini_loss,alpha)\n",
    "    stumps.append(tree)\n",
    "\n",
    "    x_new=x_val.T\n",
    "    acc=[[0,0] for i in range(2) ]\n",
    "    acc={\"-1\":[0,0],\"1\":[0,0]}\n",
    "    for i in range(x_new.shape[0]):\n",
    "        acc[str(y_val[i])][1]+=1\n",
    "\n",
    "        if (prediction(stumps, x_new[i])==y_val[i]):\n",
    "            acc[str(y_val[i])][0]+=1\n",
    "\n",
    "    # cate=[-1,1]\n",
    "    cnt=0\n",
    "    tot=0\n",
    "    for i in acc:\n",
    "        t=acc[i]\n",
    "        tot+=((t[0]/t[1])*100)\n",
    "        # print(cate[cnt],\":\",(t[0]/t[1])*100,\"%\")\n",
    "        cnt+=1\n",
    "    minimumm=min(minimumm,tot/2)\n",
    "    \n",
    "    print(\"Total Accuracy\",tot/2,\"%\")\n",
    "    values.append(tot/2)\n",
    "    if (tot/2>maximum_accuracy):\n",
    "        maximum_accuracy=tot/2\n",
    "        max_num_tree=st\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Accuracy  99.69999999999999 at the  91 th tree\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum Accuracy \",maximum_accuracy,\"at the \",max_num_tree+1,\"th tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAAHHCAYAAACr0swBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAABcyklEQVR4nO3de1xUdf4/8NfMMAwwgkogDIh4QUVN3fUSixVpAd4qKQtS++GN+qa4oHzza2xramaWttputei2meUFy1toXpAyNNZLSmriFdQ0FDFTriIMw+f3h8uJkZlxBoEBzuv5ePjYM+ccPpzzdjy99vP5zGcUQggBIiIiIjKitPcFEBERETVFDElEREREJjAkEREREZnAkERERERkAkMSERERkQkMSUREREQmMCQRERERmcCQRERERGQCQxIRERGRCQxJREQtVMeOHTFhwgTpdXp6OhQKBdLT0+12TUTNCUMSUQv1z3/+EwqFAkFBQfa+FLqHn3/+GQqFAu+99569L4WIanCw9wUQUcNYs2YNOnbsiB9++AE5OTkICAiw9yWRnYWEhKCsrAyOjo72vhSiZoE9SUQt0IULF7Bv3z4sWbIEnp6eWLNmjb0vyazS0lJ7X4JsKJVKODk5Qanko5/IGvyXQtQCrVmzBm3btsXIkSPx3HPPmQ1JBQUFmDFjBjp27AiNRoP27dsjOjoa169fl865ffs25s6di27dusHJyQk6nQ7PPvsszp07B8D8PJfqIaSVK1dK+yZMmIBWrVrh3LlzGDFiBFxdXTFu3DgAwPfff4/nn38eHTp0gEajgZ+fH2bMmIGysrJa13369GlERkbC09MTzs7O6N69O15//XUAwHfffQeFQoHNmzfX+rm1a9dCoVBg//79Jutx+PBhKBQKfPbZZ7WOpaamQqFQ4OuvvwYAFBcXY/r06VLt2rVrh7CwMPz4448m27bVypUroVAo8J///AcJCQnw9PSEVqvFM888g19//dXoXCEE3nrrLbRv3x4uLi4YMmQITpw4UatNc39XBw8exIgRI9C2bVtotVr06dMHf//7343OOX36NJ577jm4u7vDyckJAwYMwJYtW4zO0ev1mDdvHrp27QonJyc88MADeOSRR5CWllYvNSFqbBxuI2qB1qxZg2effRaOjo4YM2YMkpKScOjQIQwcOFA6p6SkBI8++ihOnTqFSZMmoV+/frh+/Tq2bNmC3NxceHh4wGAw4Mknn8S3336LF154AfHx8SguLkZaWhqysrLQpUsXm6+tsrISQ4cOxSOPPIL33nsPLi4uAID169fj1q1bmDJlCh544AH88MMP+OCDD5Cbm4v169dLP//TTz/h0UcfhVqtxssvv4yOHTvi3Llz2Lp1KxYsWIDBgwfDz88Pa9aswTPPPFOrLl26dEFwcLDJaxswYAA6d+6ML7/8EuPHjzc69sUXX6Bt27YYOnQoAOCVV17Bhg0bMG3aNPTs2RO//fYbMjIycOrUKfTr18/mupjz5z//GW3btsWcOXPw888/4/3338e0adPwxRdfSOe88cYbeOuttzBixAiMGDECP/74I8LDw1FRUXHP9tPS0vDkk09Cp9MhPj4e3t7eOHXqFL7++mvEx8cDAE6cOIGHH34Yvr6+eO2116DVavHll18iIiICGzdulOo8d+5cLFy4EDExMXjooYdQVFSEw4cP48cff0RYWFi91YSo0QgialEOHz4sAIi0tDQhhBBVVVWiffv2Ij4+3ui8N954QwAQmzZtqtVGVVWVEEKIFStWCABiyZIlZs/57rvvBADx3XffGR2/cOGCACA+/fRTad/48eMFAPHaa6/Vau/WrVu19i1cuFAoFApx8eJFaV9ISIhwdXU12lfzeoQQIjExUWg0GlFQUCDtu3btmnBwcBBz5syp9XtqSkxMFGq1Wty4cUPaV15eLtq0aSMmTZok7WvdurWIjY212Ja1qmu1ePFiad+nn34qAIjQ0FCje5sxY4ZQqVTSvV27dk04OjqKkSNHGp33l7/8RQAQ48ePl/bd/XdVWVkpOnXqJPz9/cXNmzeNrqlmW0888YTo3bu3uH37ttHxQYMGia5du0r7+vbtK0aOHHlftSBqSjjcRtTCrFmzBl5eXhgyZAgAQKFQICoqCuvWrYPBYJDO27hxI/r27Vurt6X6Z6rP8fDwwJ///Gez59TFlClTau1zdnaWtktLS3H9+nUMGjQIQggcOXIEAPDrr79i7969mDRpEjp06GD2eqKjo1FeXo4NGzZI+7744gtUVlbixRdftHhtUVFR0Ov12LRpk7Rv165dKCgoQFRUlLSvTZs2OHjwIK5cuWLlXdfNyy+/bHRvjz76KAwGAy5evAgA+Oabb1BRUYE///nPRudNnz79nm0fOXIEFy5cwPTp09GmTRujY9Vt3bhxA7t370ZkZCSKi4tx/fp1XL9+Hb/99huGDh2K7OxsXL58GcCdmpw4cQLZ2dn3eddETQNDElELYjAYsG7dOgwZMgQXLlxATk4OcnJyEBQUhPz8fHz77bfSuefOncODDz5osb1z586he/fucHCov5F5BwcHtG/fvtb+S5cuYcKECXB3d0erVq3g6emJxx57DABQWFgIADh//jwA3PO6AwMDMXDgQKO5WGvWrMGf/vSne37Kr2/fvggMDDQazvriiy/g4eGBxx9/XNq3aNEiZGVlwc/PDw899BDmzp0rXV99ujsMtm3bFgBw8+ZNAJDCUteuXY3O8/T0lM41p3pemaV65uTkQAiB2bNnw9PT0+jPnDlzAADXrl0DALz55psoKChAt27d0Lt3b8ycORM//fSTtbdK1ORwThJRC7J7927k5eVh3bp1WLduXa3ja9asQXh4eL3+TnM9SjV7rWrSaDS1Pl1lMBgQFhaGGzduYNasWQgMDIRWq8Xly5cxYcIEVFVV2Xxd0dHRiI+PR25uLsrLy3HgwAF8+OGHVv1sVFQUFixYgOvXr8PV1RVbtmzBmDFjjMJiZGQkHn30UWzevBm7du3C4sWL8e6772LTpk0YPny4zddrjkqlMrlfCFFvv8OS6tq/+uqr0nysu1UHz5CQEJw7dw4pKSnYtWsX/v3vf2Pp0qVYtmwZYmJiGuV6ieoTQxJRC7JmzRq0a9cOH330Ua1jmzZtwubNm7Fs2TI4OzujS5cuyMrKsthely5dcPDgQej1eqjVapPnVPdWFBQUGO2v7uGwxvHjx3H27Fl89tlniI6Olvbf/amozp07A8A9rxsAXnjhBSQkJCA5ORllZWVQq9VGw2WWREVFYd68edi4cSO8vLxQVFSEF154odZ5Op0OU6dOxdSpU3Ht2jX069cPCxYsqNeQdC/+/v4AgOzsbKk+wJ2hyereJnOqJ95nZWUhNDTU5DnVbarVarPn1OTu7o6JEydi4sSJKCkpQUhICObOncuQRM0Sh9uIWoiysjJs2rQJTz75JJ577rlaf6ZNm4bi4mLpY9ujR4/GsWPHTH5UvrqXYvTo0bh+/brJHpjqc/z9/aFSqbB3716j4//85z+tvvbq3pKavSNCiFofQ/f09ERISAhWrFiBS5cumbyeah4eHhg+fDhWr16NNWvWYNiwYfDw8LDqenr06IHevXvjiy++wBdffAGdToeQkBDpuMFgkIYAq7Vr1w4+Pj4oLy+X9l2/fh2nT5/GrVu3rPq9dREaGgq1Wo0PPvjAqAbvv//+PX+2X79+6NSpE95///1aIbe6rXbt2mHw4MFYvnw58vLyarVRczmC3377zehYq1atEBAQYFQTouaEPUlELcSWLVtQXFyMp59+2uTxP/3pT9LCklFRUZg5cyY2bNiA559/HpMmTUL//v1x48YNbNmyBcuWLUPfvn0RHR2Nzz//HAkJCfjhhx/w6KOPorS0FN988w2mTp2KUaNGoXXr1nj++efxwQcfQKFQoEuXLvj666+leSrWCAwMRJcuXfDqq6/i8uXLcHNzw8aNG032hPzjH//AI488gn79+uHll19Gp06d8PPPP2Pbtm04evSo0bnR0dF47rnnAADz58+3vpi405v0xhtvwMnJCZMnTzYaIiwuLkb79u3x3HPPoW/fvmjVqhW++eYbHDp0CH/729+k8z788EPMmzcP3333HQYPHmzT77eWp6cnXn31VSxcuBBPPvkkRowYgSNHjmDHjh33DIVKpRJJSUl46qmn8Ic//AETJ06ETqfD6dOnceLECaSmpgIAPvroIzzyyCPo3bs3XnrpJXTu3Bn5+fnYv38/cnNzcezYMQBAz549MXjwYPTv3x/u7u44fPiwtEwCUbNkr4/VEVH9euqpp4STk5MoLS01e86ECROEWq0W169fF0II8dtvv4lp06YJX19f4ejoKNq3by/Gjx8vHRfizkfzX3/9ddGpUyehVquFt7e3eO6558S5c+ekc3799VcxevRo4eLiItq2bSv+53/+R2RlZZlcAkCr1Zq8tpMnT4rQ0FDRqlUr4eHhIV566SVx7NixWm0IIURWVpZ45plnRJs2bYSTk5Po3r27mD17dq02y8vLRdu2bUXr1q1FWVmZNWWUZGdnCwACgMjIyKjV7syZM0Xfvn2Fq6ur0Gq1om/fvuKf//yn0Xlz5swxuTzC3SwtAXDo0CGjc00tuWAwGMS8efOETqcTzs7OYvDgwSIrK0v4+/tbXAKgWkZGhggLC5PupU+fPuKDDz4wOufcuXMiOjpaeHt7C7VaLXx9fcWTTz4pNmzYIJ3z1ltviYceeki0adNGODs7i8DAQLFgwQJRUVFh8f6JmiqFEI00+4+IqJFVVlbCx8cHTz31FD755BN7Xw4RNTOck0RELdZXX32FX3/91WgyOBGRtdiTREQtzsGDB/HTTz9h/vz58PDwqLfvUyMieWFPEhG1OElJSZgyZQratWuHzz//3N6XQ0TNFHuSiIiIiExgTxIRERGRCQxJRERERCZwMck6qqqqwpUrV+Dq6npf34ZOREREjUcIgeLiYvj4+NT6Hsm7MSTV0ZUrV+Dn52fvyyAiIqI6+OWXX9C+fXuL5zAk1ZGrqyuAO0V2c3Or17b1ej127dqF8PBws18qSnewVrZhvazHWlmPtbIN62W9hqhVUVER/Pz8pP+OW8KQVEfVQ2xubm4NEpJcXFzg5ubGf0D3wFrZhvWyHmtlPdbKNqyX9RqyVtZMleHEbSIiIiITGJKIiIiITGBIIiIiIjKBIYmIiIjIBIYkIiIiIhMYkoiIiIhMYEgiIiIiMoEhiYiIiMgEhiQiIiIiExiSiIiIiEywe0gqLi7G9OnT4e/vD2dnZwwaNAiHDh2Sjufn52PChAnw8fGBi4sLhg0bhuzsbIttDh48GAqFotafkSNHSucIIfDGG29Ap9PB2dkZoaGh92yXiIiI5MPuISkmJgZpaWlYtWoVjh8/jvDwcISGhuLy5csQQiAiIgLnz59HSkoKjhw5An9/f4SGhqK0tNRsm5s2bUJeXp70JysrCyqVCs8//7x0zqJFi/CPf/wDy5Ytw8GDB6HVajF06FDcvn27MW6biIiImji7fsFtWVkZNm7ciJSUFISEhAAA5s6di61btyIpKQnR0dE4cOAAsrKy0KtXLwBAUlISvL29kZycjJiYGJPturu7G71et24dXFxcpJAkhMD777+Pv/71rxg1ahQA4PPPP4eXlxe++uorvPDCCw11y0T3TX9LD7WL2urt0l9Lob+lBwBU6itRca0ChRcL4aDm91tbwlpZj7WyDetlAzt//69d/3YqKythMBjg5ORktN/Z2RkZGRmIiooCAKPjSqUSGo0GGRkZZkPS3T755BO88MIL0Gq1AIALFy7g6tWrCA0Nlc5p3bo1goKCsH//fpMhqby8HOXl5dLroqIiAHe+oViv11t5x9apbq++222J5Farw0mHkZaQhqiUKBRcLEBqXCqe3/Q8Sq6WYMfUHRj95Wjcvnkb2/5nG55Nfhb6W3psmbClVjsncdIOV988sVbWY61sw3rdW4/ne0AzTlOvz3hb2rJrSHJ1dUVwcDDmz5+PHj16wMvLC8nJydi/fz8CAgIQGBiIDh06IDExEcuXL4dWq8XSpUuRm5uLvLw8q37HDz/8gKysLHzyySfSvqtXrwIAvLy8jM718vKSjt1t4cKFmDdvXq39u3btgouLi7W3bJO0tLQGabclkkutLn51EcIgsGfdHpTnlUMYBPau2wv9b3oIg8D3X34PQ5EBwiCQ8WUGDLcNd35QCSgcFPa9eCIiG129fhX+8K/XZ/ytW7esPtfu/XyrVq3CpEmT4OvrC5VKhX79+mHMmDHIzMyEWq3Gpk2bMHnyZLi7u0OlUiE0NBTDhw+HEMKq9j/55BP07t0bDz300H1dZ2JiIhISEqTXRUVF8PPzQ3h4ONzc3O6r7bvp9XqkpaUhLCwMarWd+xqbOLnVKuWLFNzETXQN6Iqbipu4gRsI6BKAYqdi/Ibf0KVTF5T9VobruI7OHTujorQC13EdD7/2MB6b+5js6nU/WCvrsVa2Yb2s1xC1qh4JsobdQ1KXLl2wZ88elJaWoqioCDqdDlFRUejcuTMAoH///jh69CgKCwtRUVEBT09PBAUFYcCAAfdsu7S0FOvWrcObb75ptN/b2xvAnU/O6XQ6aX9+fj7+8Ic/mGxLo9FAo9HU2q9WqxvsTd6Qbbc0sqlV1Z3/UQjFPbchfj9frTGuj2zqVQ9YK+uxVrZhvaxXn7WypR27f7qtmlarhU6nw82bN5GamipNqK7WunVreHp6Ijs7G4cPH6513JT169ejvLwcL774otH+Tp06wdvbG99++620r6ioCAcPHkRwcHD93BBRAxCGOz2oVYYqq7arDHdSkkLFoTYiIlvZvScpNTUVQgh0794dOTk5mDlzJgIDAzFx4kQAd4KOp6cnOnTogOPHjyM+Ph4REREIDw+X2oiOjoavry8WLlxo1PYnn3yCiIgIPPDAA0b7FQoFpk+fjrfeegtdu3ZFp06dMHv2bPj4+CAiIqLB75morqoqq6T/tWZbVN4JTEpVk/n/Q0REzYbdQ1JhYSESExORm5sLd3d3jB49GgsWLJC6w/Ly8pCQkCANjUVHR2P27NlGbVy6dAlKpfF/BM6cOYOMjAzs2rXL5O/9v//7P5SWluLll19GQUEBHnnkEezcubPWJ+2ImpLqniFhEDZtKx0YkoiIbGX3kBQZGYnIyEizx+Pi4hAXF2exjfT09Fr7unfvbnFyt0KhwJtvvllrvhJRUyb1Ehnu6jGqHmIzs83hNiIi2/H/XhI1IybDUM3AZGabPUlERLbjk5OoGTEaSqs5D8lQe1tU1hhu45wkIiKb8clJ1IyYCkY1A5NReGJPEhHRfeGTk6gZMfVRf2EQ99zmnCQiItsxJBE1I1KPUaWwaTkA9iQREdmOT06iZkSae1RjoUhrtjkniYjIdnxyEjUjJucecQkAIqIGwZBE1IyYCkDmJm7X3OZwGxGR7fjkJGpGzC0maWoJgJrbHG4jIrIdn5xEzYjJMMTFJImIGgSfnETNiKkhNs5JIiJqGAxJRM2IqWDEOUlERA2DT06iZsTkEBvnJBERNQg+OYmaEZPBiHOSiIgaBJ+cRM0I5yQRETUehiSiZqRmL5FRYKr5ZbcmttmTRERkOz45iZqROi8BwDlJREQ245OTqBnh15IQETUehiSiZoRfS0JE1Hj45CRqRvi1JEREjYdPTqJmhF9LQkTUePjkJGomRJUAxH+3OSeJiKjBMSQRNRPVPUcAYNAbjAMTlwAgIqp3fHISNRPVvUIAYKgwSNt39x4ZzVXiEgBERHXGJydRM1EdeADAUP57SDIbmGosOMmeJCIi2/HJSdRM1BxuqyyvNLlt0BsHpupgxTlJRES2Y0giaibM9iSZ2a7S/34+e5KIiGzHJydRM2FuTpI125yTRERkOz45iZqJmj1JtuJwGxGR7RiSiJqJmnOSbMXhNiIi2/HJSdRM3E9PEofbiIhsxycnUTNRc06SrdiTRERkOz45iZqJ+xluUyg5J4mIyFZ2DUnFxcWYPn06/P394ezsjEGDBuHQoUPS8fz8fEyYMAE+Pj5wcXHBsGHDkJ2dfc92CwoKEBsbC51OB41Gg27dumH79u3ScYPBgNmzZ6NTp05wdnZGly5dMH/+fAhR9/+nTtTQ6jrcxl4kIqK6cbDnL4+JiUFWVhZWrVoFHx8frF69GqGhoTh58iR8fHwQEREBtVqNlJQUuLm5YcmSJdJxrVZrss2KigqEhYWhXbt22LBhA3x9fXHx4kW0adNGOufdd99FUlISPvvsM/Tq1QuHDx/GxIkT0bp1a8TFxTXS3RPZpq7DbfxkGxFR3dgtJJWVlWHjxo1ISUlBSEgIAGDu3LnYunUrkpKSEB0djQMHDiArKwu9evUCACQlJcHb2xvJycmIiYkx2e6KFStw48YN7Nu3D2q1GgDQsWNHo3P27duHUaNGYeTIkdLx5ORk/PDDDw10t0T3jz1JRESNy25Pz8rKShgMBjg5ORntd3Z2RkZGBsrLywHA6LhSqYRGo0FGRobZdrds2YLg4GDExsbCy8sLDz74IN5++20YDL8vrDdo0CB8++23OHv2LADg2LFjyMjIwPDhw+vzFonqVV3nJPGTbUREdWO3niRXV1cEBwdj/vz56NGjB7y8vJCcnIz9+/cjICAAgYGB6NChAxITE7F8+XJotVosXboUubm5yMvLM9vu+fPnsXv3bowbNw7bt29HTk4Opk6dCr1ejzlz5gAAXnvtNRQVFSEwMBAqlQoGgwELFizAuHHjzLZbXl4uBTcAKCoqAgDo9Xro9fp6qgqkNmv+L5knp1pV3K6o088pVIpadZJDve4Xa2U91so2rJf1GqJWtrSlEHacrXzu3DlMmjQJe/fuhUqlQr9+/dCtWzdkZmbi1KlTyMzMxOTJk3Hs2DGoVCqEhoZCqVRCCIEdO3aYbLNbt264ffs2Lly4AJVKBQBYsmQJFi9eLIWrdevWYebMmVi8eDF69eqFo0ePYvr06ViyZAnGjx9vst25c+di3rx5tfavXbsWLi4u9VQRIvNKTpYg5y85Nv+cQ2sHPPjZgw1wRUREzc+tW7cwduxYFBYWws3NzeK5dg1J1UpLS1FUVASdToeoqCiUlJRg27Zt0vHCwkJUVFTA09MTQUFBGDBgAD766COTbT322GNQq9X45ptvpH07duzAiBEjUF5eDkdHR/j5+eG1115DbGysdM5bb72F1atX4/Tp0ybbNdWT5Ofnh+vXr9+zyLbS6/VIS0tDWFiYNK+KTJNTrS7uuYg1YWts/jmttxbxl+IByKte94u1sh5rZRvWy3oNUauioiJ4eHhYFZLs+um2alqtFlqtFjdv3kRqaioWLVpkdLx169YAgOzsbBw+fBjz588329bDDz+MtWvXoqqqCkrlnbkYZ8+ehU6ng6OjI4A7KbL6WDWVSoWqKvNzPjQaDTQaTa39arW6wd7kDdl2SyOHWikVdZtbpHJQ1aqNHOpVX1gr67FWtmG9rFeftbKlHbvO6ExNTcXOnTtx4cIFpKWlYciQIQgMDMTEiRMBAOvXr0d6ejrOnz+PlJQUhIWFISIiAuHh4VIb0dHRSExMlF5PmTIFN27cQHx8PM6ePYtt27bh7bffNuo1euqpp7BgwQJs27YNP//8MzZv3owlS5bgmWeeabybJ7IRlwAgImpcdu1JKiwsRGJiInJzc+Hu7o7Ro0djwYIFUsrLy8tDQkIC8vPzodPpEB0djdmzZxu1cenSJaNeIT8/P6SmpmLGjBno06cPfH19ER8fj1mzZknnfPDBB5g9ezamTp2Ka9euwcfHB//zP/+DN954o3FunKgOuAQAEVHjsmtIioyMRGRkpNnjcXFx91zcMT09vda+4OBgHDhwwOzPuLq64v3338f7779v7aUS2R2XACAialx8ehI1E+xJIiJqXHx6EjUTnJNERNS4GJKImgn2JBERNS4+PYmaCc5JIiJqXHx6EjUTde1J4nAbEVHdMCQRNRN1nZPE4TYiorrh05OomajznCQOtxER1QmfnkTNRJ3nJLEniYioTvj0JGomuAQAEVHjYkgiaia4BAARUePi05OomeASAEREjYtPT6Jmgj1JRESNi09PomaCc5KIiBoXQxJRM8GeJCKixsWnJ1EzwTlJRESNi09PomaCX0tCRNS4GJKImgl+LQkRUePi05OomWBPEhFR42JIImom+LUkRESNi09PomaizsNtnLhNRFQnfHoSNRNcAoCIqHHx6UnUTNR1uI1zkoiI6oYhiaiZYE8SEVHj4tOTqJngnCQiosbFpydRM8ElAIiIGhdDElEzwcUkiYgaF5+eRM1EneckcbiNiKhO+PQkaiaqP92m0qikfdZssyeJiKhu+PQkaiaqh9scNA7SPmu2OSeJiKhuGJKImonq4Tb2JBERNQ4+PYmaierhNlt7kjgniYiobvj0JGom2JNERNS4+PQkaiY4J4mIqHExJBE1E+xJIiJqXHZ9ehYXF2P69Onw9/eHs7MzBg0ahEOHDknH8/PzMWHCBPj4+MDFxQXDhg1Ddnb2PdstKChAbGwsdDodNBoNunXrhu3btxudc/nyZbz44ot44IEH4OzsjN69e+Pw4cP1fo9E9UVaAsCxRhiyYptzkoiI6sbh3qc0nJiYGGRlZWHVqlXw8fHB6tWrERoaipMnT8LHxwcRERFQq9VISUmBm5sblixZIh3XarUm26yoqEBYWBjatWuHDRs2wNfXFxcvXkSbNm2kc27evImHH34YQ4YMwY4dO+Dp6Yns7Gy0bdu2ke6cyHbVPUkcbiMiahx2C0llZWXYuHEjUlJSEBISAgCYO3cutm7diqSkJERHR+PAgQPIyspCr169AABJSUnw9vZGcnIyYmJiTLa7YsUK3LhxA/v27YNarQYAdOzY0eicd999F35+fvj000+lfZ06dWqAuySqP9VzkjjcRkTUOOz29KysrITBYICTk5PRfmdnZ2RkZKC8vBwAjI4rlUpoNBpkZGSYbXfLli0IDg5GbGwsvLy88OCDD+Ltt9+GwWAwOmfAgAF4/vnn0a5dO/zxj3/Exx9/XM93SFS/6tqTxOE2IqK6sVtPkqurK4KDgzF//nz06NEDXl5eSE5Oxv79+xEQEIDAwEB06NABiYmJWL58ObRaLZYuXYrc3Fzk5eWZbff8+fPYvXs3xo0bh+3btyMnJwdTp06FXq/HnDlzpHOSkpKQkJCAv/zlLzh06BDi4uLg6OiI8ePHm2y3vLxcCm4AUFRUBADQ6/XQ6/X1WBlI7dV3uy2RnGplqLwT9BXq34fPrNmuQlWtOsmhXveLtbIea2Ub1st6DVErW9pSCCHq9tXi9eDcuXOYNGkS9u7dC5VKhX79+qFbt27IzMzEqVOnkJmZicmTJ+PYsWNQqVQIDQ2FUqmEEAI7duww2Wa3bt1w+/ZtXLhwASrVnSGHJUuWYPHixVK4cnR0xIABA7Bv3z7p5+Li4nDo0CHs37/fZLtz587FvHnzau1fu3YtXFxc7rcURPd0atoplOeWo21IW9zcexMArNrulNgJrYNa2+eiiYiamFu3bmHs2LEoLCyEm5ubxXPtOnG7S5cu2LNnD0pLS1FUVASdToeoqCh07twZANC/f38cPXoUhYWFqKiogKenJ4KCgjBgwACzbep0OqjVaikgAUCPHj1w9epVVFRUwNHRETqdDj179jT6uR49emDjxo1m201MTERCQoL0uqioCH5+fggPD79nkW2l1+uRlpaGsLAwaV4VmSanWl10uohylMOvk58UgKzZHhg0EAEjAgDIq173i7WyHmtlG9bLeg1Rq+qRIGvYNSRV02q10Gq1uHnzJlJTU7Fo0SKj461b3/l/wdnZ2Th8+DDmz59vtq2HH34Ya9euRVVVFZTKO3Mxzp49C51OB0dHR+mcM2fOGP3c2bNn4e/vb7ZdjUYDjUZTa79arW6wN3lDtt3SyKFW1RO31U6/36dV25ratZFDveoLa2U91so2rJf16rNWtrRj1xmdqamp2LlzJy5cuIC0tDQMGTIEgYGBmDhxIgBg/fr1SE9Px/nz55GSkoKwsDBEREQgPDxcaiM6OhqJiYnS6ylTpuDGjRuIj4/H2bNnsW3bNrz99tuIjY2VzpkxYwYOHDiAt99+Gzk5OVi7di3+9a9/GZ1D1NRwMUkiosZl156kwsJCJCYmIjc3F+7u7hg9ejQWLFggpby8vDwkJCQgPz8fOp0O0dHRmD17tlEbly5dknqMAMDPzw+pqamYMWMG+vTpA19fX8THx2PWrFnSOQMHDsTmzZuRmJiIN998E506dcL777+PcePGNc6NE9UBv5aEiKhx2TUkRUZGIjIy0uzxuLg4xMXFWWwjPT291r7g4GAcOHDA4s89+eSTePLJJ626TqKmoM49SVwCgIioTvj0JGomqr+WxOZ1kjjcRkRUJ3x6EjUTde1J4nAbEVHdMCQRNRN1nZPEniQiorrh05OomeCcJCKixsWnJ1EzwTlJRESNi09PomaieriNc5KIiBoHQxJRMyCEgKjinCQiosbEpydRM1DdiwRwThIRUWPh05OoGaietA0AKkeVTdvsSSIiqhs+PYmagepJ2wC/loSIqLEwJBE1A0Y9SRxuIyJqFHx6EjUDNeckceI2EVHj4NOTqBkwOyeJSwAQETUYhiSiZqB6TpJCqYBS/fs/25qBiT1JRET1y+HepxC1bPpbeqhd1NJrIQQqb1dC7ayutV18udhoEnVjKckrAXCnV0iaY6QAVGrOSSIiaigMSSRr+cfz8fHAjxGcEIwn3n4CALBl0hac3HAS085Mw3dzvkNWchZiT8Xi+7e/R+ayTLter1KllHqGam4DdwUmRw63ERHdL4YkkrX8Y/kwlBtw+YfL0r7cg7moKKnA9dPXcfngZehL9fj15K+4fPDOOUq10m69M73H9Yarryv8H/NHm45t0Mq7FTo93gmtdK3g4umCzmGd4ezuDBcPFwQMD4DaRW009EZERNbj05NkTYj/fmpM1NxZ45iose+/22O2jkHA0IBGukLTJqRPkLajv42Wtv/frv8nbY/bPq4xL4mIqMXhZAWStervQ6v+37v3mdpWKDl8RUQkBwxJJGsMSUREZA5DEslanUKSgiGJiEgOGJJI3mrOP8Jd2+L3bSGEtM2eJCIieWBIIlnjcBsREZnDkESyxpBERETmMCSRrFlcAqDK9BIAYEYiIpIFhiSSNYs9SYI9SUREcsaQRLLG4TYiIjKHIYlkjSGJiIjMYUgieavLEgBcJ4mISBYYkkjW2JNERETmMCSRrDEkERGROQxJJGvScFqNkMQlAIiICGBIIpmTwpGovY9LABARyRtDEskah9uIiMgcu4ek4uJiTJ8+Hf7+/nB2dsagQYNw6NAh6Xh+fj4mTJgAHx8fuLi4YNiwYcjOzr5nuwUFBYiNjYVOp4NGo0G3bt2wfft2k+e+8847UCgUmD59en3dFjUTDElERGSOg70vICYmBllZWVi1ahV8fHywevVqhIaG4uTJk/Dx8UFERATUajVSUlLg5uaGJUuWSMe1Wq3JNisqKhAWFoZ27dphw4YN8PX1xcWLF9GmTZta5x46dAjLly9Hnz59GvhOqUmysASAqBJcAoCISMbsGpLKysqwceNGpKSkICQkBAAwd+5cbN26FUlJSYiOjsaBAweQlZWFXr16AQCSkpLg7e2N5ORkxMTEmGx3xYoVuHHjBvbt2we1Wg0A6NixY63zSkpKMG7cOHz88cd46623GuYmqUmz1JMEYbpXiT1JRETyYNeQVFlZCYPBACcnJ6P9zs7OyMjIQFRUFAAYHVcqldBoNMjIyDAbkrZs2YLg4GDExsYiJSUFnp6eGDt2LGbNmgWVSiWdFxsbi5EjRyI0NPSeIam8vBzl5eXS66KiIgCAXq+HXq+37cbvobq9+m63JbrfWlXqKwHcCUHVbVSHoUp9peltQ2Wz/bvhe8t6rJX1WCvbsF7Wa4ha2dKWXUOSq6srgoODMX/+fPTo0QNeXl5ITk7G/v37ERAQgMDAQHTo0AGJiYlYvnw5tFotli5ditzcXOTl5Zlt9/z589i9ezfGjRuH7du3IycnB1OnToVer8ecOXMAAOvWrcOPP/5oNP/JkoULF2LevHm19u/atQsuLi51K8A9pKWlNUi7LVFda5V39s776FbpLWnOWpWhCgBw/KfjMBgMAIATWSekQLVnzx5ozmru95Ltiu8t67FW1mOtbMN6Wa8+a3Xr1i2rz7X7nKRVq1Zh0qRJ8PX1hUqlQr9+/TBmzBhkZmZCrVZj06ZNmDx5Mtzd3aFSqRAaGorhw4cbzSG5W1VVFdq1a4d//etfUKlU6N+/Py5fvozFixdjzpw5+OWXXxAfH4+0tLRavVjmJCYmIiEhQXpdVFQEPz8/hIeHw83N7b7rUJNer0daWhrCwsKk4UIy7X5rtefgHuQjH05OThgxYgQA4BiOQUCgV69eyEMeDDCgZ4+euKa8hipUYcjjQ9CmU5t6vpPGwfeW9Vgr67FWtmG9rNcQtaoeCbKG3UNSly5dsGfPHpSWlqKoqAg6nQ5RUVHo3LkzAKB///44evQoCgsLUVFRAU9PTwQFBWHAgAFm29TpdFCr1UZDaz169MDVq1dRUVGBzMxMXLt2Df369ZOOGwwG7N27Fx9++CHKy8uNfhYANBoNNJravQdqtbrB3uQN2XZLU9daSZOwqyD9fPWwmlKhNLmt1jT/vxe+t6zHWlmPtbIN62W9+qyVLe3YfQmAalqtFjqdDjdv3kRqaipGjRpldLx169bw9PREdnY2Dh8+XOt4TQ8//DBycnJQVVUl7Tt79ix0Oh0cHR3xxBNP4Pjx4zh69Kj0Z8CAARg3bhyOHj1aKyBRy3X3xG2jT7lxCQAiIlmze09SamoqhBDo3r07cnJyMHPmTAQGBmLixIkAgPXr18PT0xMdOnTA8ePHER8fj4iICISHh0ttREdHw9fXFwsXLgQATJkyBR9++CHi4+Px5z//GdnZ2Xj77bcRFxcH4M5cqAcffNDoOrRaLR544IFa+6mFq/kVJDVeV+/jEgBERPJl95BUWFiIxMRE5Obmwt3dHaNHj8aCBQuk7rC8vDwkJCQgPz8fOp0O0dHRmD17tlEbly5dglL5e6eYn58fUlNTMWPGDPTp0we+vr6Ij4/HrFmzGvXeqOm7uwep1lIANUIUe5KIiOTF7iEpMjISkZGRZo/HxcVJPUDmpKen19oXHByMAwcOWH0dptqglq/WcFvNkGQwPfTGkEREJA9NZk4SkT1YCknVSwHcOQCpV4khiYhIHhiSSNakidomvp6kqvL3kGQUmJiRiIhkgSGJZM3a4baagYk9SURE8sCQRLJmcbitRjCqGZgYkoiI5IEhieTNwhIANYfYam5zCQAiInlgSCJZs7QEgNGcJA63ERHJDkMSyZrVSwBwuI2ISHYYkkjWrF0CwGi4jSGJiEgWGJJI1u4eZjO7BEAllwAgIpIbhiSStbsnbHMJACIiqlankFRZWYlvvvkGy5cvR3FxMQDgypUrKCkpqdeLI2pw/80+XAKAiIjuZvN3t128eBHDhg3DpUuXUF5ejrCwMLi6uuLdd99FeXk5li1b1hDXSdQgjHqOhLBuThKXACAikgWbe5Li4+MxYMAA3Lx5E87OztL+Z555Bt9++229XhxRQ6s5B6nm97MBgKgUJrfZk0REJA829yR9//332LdvHxwdHY32d+zYEZcvX663CyNqDEY9SVUWepI4cZuISHZs7kmqqqqCwWCotT83Nxeurq71clFEjcViSDL1BbcKDrcREcmFzSEpPDwc77//vvRaoVCgpKQEc+bMwYgRI+rz2ogaXs3htSphNPxmajFJBiQiIvmwebjtb3/7G4YOHYqePXvi9u3bGDt2LLKzs+Hh4YHk5OSGuEaiBmNx4raJdZI4H4mISD5sDknt27fHsWPHsG7dOvz0008oKSnB5MmTMW7cOKOJ3ETNgdVzkgwMSUREcmNzSAIABwcHvPjii/V9LUSNzlJIMjncxpBERCQbNoekzz//3OLx6OjoOl8MUWMzmoNUJYzmKJn8WhJmJCIi2bA5JMXHxxu91uv1uHXrFhwdHeHi4sKQRM1KzZ4jCPMrbnNOEhGR/Nj86babN28a/SkpKcGZM2fwyCOPcOI2NTuck0RERObUyxfcdu3aFe+8806tXiaiJo9LABARkRn1EpKAO5O5r1y5Ul/NETUKLgFARETm2DwnacuWLUavhRDIy8vDhx9+iIcffrjeLoyoMXC4jYiIzLE5JEVERBi9VigU8PT0xOOPP46//e1v9XVdRI2CSwAQEZE5Noekqqqqe59E1ExwCQAiIjKn3uYkETVHXAKAiIjMsaonKSEhweoGlyxZUueLIWpsnJNERETmWBWSjhw5YlVj/Hg0NTc2z0nie5yISDasCknfffddQ18HkX1YWCeJw21ERPLGOUkkaxbXSeJwGxGRrNn86TYAOHz4ML788ktcunQJFRUVRsc2bdpULxdG1Bi4BAAREZljc0/SunXrMGjQIJw6dQqbN2+GXq/HiRMnsHv3brRu3bohrpGowXAJACIiMsfmkPT2229j6dKl2Lp1KxwdHfH3v/8dp0+fRmRkJDp06GDzBRQXF2P69Onw9/eHs7MzBg0ahEOHDknH8/PzMWHCBPj4+MDFxQXDhg1Ddnb2PdstKChAbGwsdDodNBoNunXrhu3bt0vHFy5ciIEDB8LV1RXt2rVDREQEzpw5Y/P1U/PGJQCIiMgcm0PSuXPnMHLkSACAo6MjSktLoVAoMGPGDPzrX/+y+QJiYmKQlpaGVatW4fjx4wgPD0doaCguX74MIQQiIiJw/vx5pKSk4MiRI/D390doaChKS0vNtllRUYGwsDD8/PPP2LBhA86cOYOPP/4Yvr6+0jl79uxBbGwsDhw4gLS0NOj1eoSHh1tsl1oeLgFARETm2DwnqW3btiguLgYA+Pr6IisrC71790ZBQQFu3bplU1tlZWXYuHEjUlJSEBISAgCYO3cutm7diqSkJERHR+PAgQPIyspCr169AABJSUnw9vZGcnIyYmJiTLa7YsUK3LhxA/v27YNarQYAdOzY0eicnTt3Gr1euXIl2rVrh8zMTOlaqOXjnCQiIjLH6pCUlZWFBx98ECEhIUhLS0Pv3r3x/PPPIz4+Hrt370ZaWhqeeOIJm355ZWUlDAYDnJycjPY7OzsjIyMDUVFRAGB0XKlUQqPRICMjw2xI2rJlC4KDgxEbG4uUlBR4enpi7NixmDVrFlQqlcmfKSwsBAC4u7ubPF5eXo7y8nLpdVFREQBAr9dDr9dbecfWqW6vvtttie63VjVDkb5cj0p9pfTa5Jyk+/hdTQHfW9ZjrazHWtmG9bJeQ9TKlrYUoubMVQuUSiUGDhyIiIgIvPjii/Dz80NVVRUWLVqEffv2oWvXrvjrX/+Ktm3b2nSxgwYNgqOjI9auXQsvLy8kJydj/PjxCAgIQFZWFgICAhAUFITly5dDq9Vi6dKleO211xAeHo7U1FSTbQYGBuLnn3/GuHHjMHXqVOTk5GDq1KmIi4vDnDlzap1fVVWFp59+GgUFBcjIyDDZ5ty5czFv3rxa+9euXQsXFxeb7pmajjMzzqDsQhkAoPvS7tDf0OP8/PMAAIWDAqJSGG07+Tkh8INAu10vERHdn1u3bmHs2LEoLCyEm5ubxXOtDknff/89Pv30U2zYsAFVVVUYPXo0YmJi8Oijj97XxZ47dw6TJk3C3r17oVKp0K9fP3Tr1g2ZmZk4deoUMjMzMXnyZBw7dgwqlQqhoaFQKpUQQmDHjh0m2+zWrRtu376NCxcuSD1HS5YsweLFi5GXl1fr/ClTpmDHjh3IyMhA+/btTbZpqifJz88P169fv2eRbaXX65GWloawsDBpuJBMu99a/bv/v3Ht+DUAwKSDk1CSV4IvI74EcGdorbqnqXrbs5cnXjryUv3dQCPje8t6rJX1WCvbsF7Wa4haFRUVwcPDw6qQZPVw26OPPopHH30UH3zwAb788kusXLkSjz32GAICAjB58mSMHz8e3t7eNl9sly5dsGfPHpSWlqKoqAg6nQ5RUVHo3LkzAKB///44evQoCgsLUVFRAU9PTwQFBWHAgAFm29TpdFCr1UZDaz169MDVq1dRUVEBR0dHaf+0adPw9ddfY+/evWYDEgBoNBpoNJpa+9VqdYO9yRuy7ZamPmqlUqmM3jN3z1cC7oSllvB3wveW9Vgr67FWtmG9rFeftbKlHZs/3abVajFx4kTs2bMHZ8+exfPPP4+PPvoIHTp0wNNPP21rc0bt6nQ63Lx5E6mpqRg1apTR8datW8PT0xPZ2dk4fPhwreM1Pfzww8jJyUFV1e/zSM6ePQudTicFJCEEpk2bhs2bN2P37t3o1KlTna+dmi9LE7dN4cRtIiL5uK+vJQkICMBf/vIX/PWvf4Wrqyu2bdtmcxupqanYuXMnLly4gLS0NAwZMgSBgYGYOHEiAGD9+vVIT0+XlgEICwtDREQEwsPDpTaio6ORmJgovZ4yZQpu3LiB+Ph4nD17Ftu2bcPbb7+N2NhY6ZzY2FisXr0aa9euhaurK65evYqrV6+irKzsPipCzY2ldZJMYUgiIpKPOn0tCQDs3bsXK1aswMaNG6FUKhEZGYnJkyfb3E5hYSESExORm5sLd3d3jB49GgsWLJC6w/Ly8pCQkID8/HzodDpER0dj9uzZRm1cunQJSuXvec/Pzw+pqamYMWMG+vTpA19fX8THx2PWrFnSOUlJSQCAwYMHG7X16aefYsKECTbfBzVP7EkiIiJzbApJV65cwcqVK7Fy5Urk5ORg0KBB+Mc//oHIyEhotdo6XUBkZCQiIyPNHo+Li0NcXJzFNtLT02vtCw4OxoEDB8z+jJXz1amlq9mRVCXu+b5QKBiSiIjkwuqQNHz4cHzzzTfw8PBAdHQ0Jk2ahO7duzfktRE1OKOeJMGeJCIi+p3VIUmtVmPDhg148sknzS7ISNTccLiNiIjMsTokbdmypSGvg8gubA1JYEYiIpKN+/p0G1FzV3MOkqgSRnOUTGFPEhGRfDAkkaxxCQAiIjKHIYlkjXOSiIjIHIYkkjcuAUBERGYwJJGssSeJiIjMYUgiWeM6SUREZA5DEskae5KIiMgchiSSNVuXAOA6SURE8sGQRLLGJQCIiMgchiSSNQ63ERGROQxJJG9cAoCIiMxgSCJZY08SERGZw5BEssYlAIiIyByGJJI19iQREZE5DEkka1wCgIiIzGFIIlljTxIREZnDkESyxnWSiIjIHIYkkjWbe5K4BAARkWwwJJG82bpOEnuSiIhkgyGJZOvuQMQlAIiIqCaGJJKtuwMRJ24TEVFNDEkkX3flIS4BQERENTEkkWyxJ4mIiCxhSCLZqhWIuAQAERHVwJBEssWeJCIisoQhiWSr1qfbrFgCgHOSiIjkgyGJZIs9SUREZAlDEslWrZDEdZKIiKgGhiSSrzosAcCvJSEikg+GJJItDrcREZElDEkkW1wCgIiILLF7SCouLsb06dPh7+8PZ2dnDBo0CIcOHZKO5+fnY8KECfDx8YGLiwuGDRuG7Ozse7ZbUFCA2NhY6HQ6aDQadOvWDdu3bzc656OPPkLHjh3h5OSEoKAg/PDDD/V+f9R0sSeJiIgssXtIiomJQVpaGlatWoXjx48jPDwcoaGhuHz5MoQQiIiIwPnz55GSkoIjR47A398foaGhKC0tNdtmRUUFwsLC8PPPP2PDhg04c+YMPv74Y/j6+krnfPHFF0hISMCcOXPw448/om/fvhg6dCiuXbvWGLdNTQCXACAiIksc7PnLy8rKsHHjRqSkpCAkJAQAMHfuXGzduhVJSUmIjo7GgQMHkJWVhV69egEAkpKS4O3tjeTkZMTExJhsd8WKFbhx4wb27dsHtVoNAOjYsaPROUuWLMFLL72EiRMnAgCWLVuGbdu2YcWKFXjttdca6I6pKWFPEhERWWLXkFRZWQmDwQAnJyej/c7OzsjIyEBUVBQAGB1XKpXQaDTIyMgwG5K2bNmC4OBgxMbGIiUlBZ6enhg7dixmzZoFlUqFiooKZGZmIjEx0ajd0NBQ7N+/32Sb5eXlKC8vl14XFRUBAPR6PfR6fd0KYEZ1e/Xdbkt0P7XSlxv/jKHSAEOlweLPCCGa9d8L31vWY62sx1rZhvWyXkPUypa27BqSXF1dERwcjPnz56NHjx7w8vJCcnIy9u/fj4CAAAQGBqJDhw5ITEzE8uXLodVqsXTpUuTm5iIvL89su+fPn8fu3bsxbtw4bN++HTk5OZg6dSr0ej3mzJmD69evw2AwwMvLy+jnvLy8cPr0aZNtLly4EPPmzau1f9euXXBxcbm/QpiRlpbWIO22RHWpVcWvFUavz5w+A/1Ny/94zl84j9vbb9v8u5oavresx1pZj7WyDetlvfqs1a1bt6w+164hCQBWrVqFSZMmwdfXFyqVCv369cOYMWOQmZkJtVqNTZs2YfLkyXB3d4dKpUJoaCiGDx9uce5IVVUV2rVrh3/9619QqVTo378/Ll++jMWLF2POnDl1us7ExEQkJCRIr4uKiuDn54fw8HC4ubnVqU1z9Ho90tLSEBYWJg0Xkmn3U6vCS4U4iZPS664BXVFytQS/4TezPxMQEIDBIwbX9XLtju8t67FW1mOtbMN6Wa8halU9EmQNu4ekLl26YM+ePSgtLUVRURF0Oh2ioqLQuXNnAED//v1x9OhRFBYWoqKiAp6enggKCsKAAQPMtqnT6aBWq6FSqaR9PXr0wNWrV1FRUQEPDw+oVCrk5+cb/Vx+fj68vb1NtqnRaKDRaGrtV6vVDfYmb8i2W5q61MpBZfz2VyqVUNxjZrZKrWoRfyd8b1mPtbIea2Ub1st69VkrW9qx+6fbqmm1Wuh0Oty8eROpqakYNWqU0fHWrVvD09MT2dnZOHz4cK3jNT388MPIyclBVVWVtO/s2bPQ6XRwdHSEo6Mj+vfvj2+//VY6XlVVhW+//RbBwcH1f3PUJHHiNhERWWL3kJSamoqdO3fiwoULSEtLw5AhQxAYGCh96mz9+vVIT0+XlgEICwtDREQEwsPDpTaio6ONJmFPmTIFN27cQHx8PM6ePYtt27bh7bffRmxsrHROQkICPv74Y3z22Wc4deoUpkyZgtLSUun3UsvHJQCIiMgSuw+3FRYWIjExEbm5uXB3d8fo0aOxYMECqTssLy8PCQkJyM/Ph06nQ3R0NGbPnm3UxqVLl6BU/p73/Pz8kJqaihkzZqBPnz7w9fVFfHw8Zs2aJZ0TFRWFX3/9FW+88QauXr2KP/zhD9i5c2etydzUcpnqSUKVmZP/iz1JRETyYfeQFBkZicjISLPH4+LiEBcXZ7GN9PT0WvuCg4Nx4MABiz83bdo0TJs2zarrpJanVkgSHG4jIqLf2X24jcheOCeJiIgsYUgi+br7+22tmJOkUDAkERHJBUMSyRZ7koiIyBKGJJKtWoFImNh3F4YkIiL5YEgi2TK1BMDdQ3C1MCMREckGQxLJFofbiIjIEoYkki0uAUBERJYwJJFssSeJiIgsYUgi+eISAEREZAFDEskWe5KIiMgShiSSLS4BQEREljAkkWzVpSeJSwAQEckHQxLJVl3WSWJPEhGRfDAkkWxxThIREVnCkESyxTlJRERkCUMSyReXACAiIgsYkki2ONxGRESWMCSRbPFrSYiIyBKGJJIt9iQREZElDEkkW3VZAoDrJBERyQdDEskWe5KIiMgShiSSLS4BQEREljAkkXxxCQAiIrKAIYlki8NtRERkCUMSyRZDEhERWcKQRLLFdZKIiMgShiSSLS4BQEREljAkkWxxuI2IiCxhSCLZ4hIARERkCUMSyVadepK4BAARkWwwJJF81WWdJPYkERHJBkMSyRbnJBERkSUMSSRbXAKAiIgssXtIKi4uxvTp0+Hv7w9nZ2cMGjQIhw4dko7n5+djwoQJ8PHxgYuLC4YNG4bs7GyLba5cuRIKhcLoj5OTk9E5JSUlmDZtGtq3bw9nZ2f07NkTy5Yta5B7pKaJSwAQEZElDva+gJiYGGRlZWHVqlXw8fHB6tWrERoaipMnT8LHxwcRERFQq9VISUmBm5sblixZIh3XarVm23Vzc8OZM2ek13dPuE1ISMDu3buxevVqdOzYEbt27cLUqVPh4+ODp59+usHul5oODrcREZEldu1JKisrw8aNG7Fo0SKEhIQgICAAc+fORUBAAJKSkpCdnY0DBw4gKSkJAwcORPfu3ZGUlISysjIkJydbbFuhUMDb21v64+XlZXR83759GD9+PAYPHoyOHTvi5ZdfRt++ffHDDz805C1TE8KQREREltg1JFVWVsJgMNQaCnN2dkZGRgbKy8sBwOi4UqmERqNBRkaGxbZLSkrg7+8PPz8/jBo1CidOnDA6PmjQIGzZsgWXL1+GEALfffcdzp49i/Dw8Hq6O2rqqgORFHyEiX0WtomIqGWz63Cbq6srgoODMX/+fPTo0QNeXl5ITk7G/v37ERAQgMDAQHTo0AGJiYlYvnw5tFotli5ditzcXOTl5Zltt3v37lixYgX69OmDwsJCvPfeexg0aBBOnDiB9u3bAwA++OADvPzyy2jfvj0cHBygVCrx8ccfIyQkxGSb5eXlUmgDgKKiIgCAXq+HXq+vx6pAaq++222J7qdWhkoDAEDpoIShwoAqQ5U0T6l6393bBoOhWf+98L1lPdbKeqyVbVgv6zVErWxpSyHutTBMAzt37hwmTZqEvXv3QqVSoV+/fujWrRsyMzNx6tQpZGZmYvLkyTh27BhUKhVCQ0OhVCohhMCOHTus+h16vR49evTAmDFjMH/+fADAe++9h48//hjvvfce/P39sXfvXiQmJmLz5s0IDQ2t1cbcuXMxb968WvvXrl0LFxeX+ysC2cX17deR+69cKBwVEBUCzgHOqLxZCf1vemkfAKPtru90hTbQ/Fw4IiJq2m7duoWxY8eisLAQbm5uFs+1e0iqVlpaiqKiIuh0OkRFRaGkpATbtm2TjhcWFqKiogKenp4ICgrCgAED8NFHH1nd/vPPPw8HBwckJyejrKwMrVu3xubNmzFy5EjpnJiYGOTm5mLnzp21ft5UT5Kfnx+uX79+zyLbSq/XIy0tDWFhYVCr1fXadktzP7U6/M/D2DV9FxxbOaKipALe/bxRkleCkrwSaR8Ao+3x34+Hb5Bvvd9HY+F7y3qslfVYK9uwXtZriFoVFRXBw8PDqpBk90+3VdNqtdBqtbh58yZSU1OxaNEio+OtW7cGAGRnZ+Pw4cNSj5A1DAYDjh8/jhEjRgD4fYhMqTSekqVSqVBVVWWyDY1GA41GU2u/Wq1usDd5Q7bd0tSlVtV//0qH/74PBKQlAKR9d207qB1axN8J31vWY62sx1rZhvWyXn3WypZ27B6SUlNTIYRA9+7dkZOTg5kzZyIwMBATJ04EAKxfvx6enp7o0KEDjh8/jvj4eERERBhNsI6Ojoavry8WLlwIAHjzzTfxpz/9CQEBASgoKMDixYtx8eJFxMTEALizPMBjjz2GmTNnwtnZGf7+/tizZw8+//xzLFmypPGLQHYhTdJWKaTXd++rtc2J20REsmH3kFRYWIjExETk5ubC3d0do0ePxoIFC6Skl5eXh4SEBOTn50On0yE6OhqzZ882auPSpUtGvUI3b97ESy+9hKtXr6Jt27bo378/9u3bh549e0rnrFu3DomJiRg3bhxu3LgBf39/LFiwAK+88krj3DjZXXUgUqqU0uu79929zZBERCQfdg9JkZGRiIyMNHs8Li4OcXFxFttIT083er106VIsXbrU4s94e3vj008/tfo6qeWRAlGN4bZa++7aZkgiIpIPu38tCZHd3DX/SFQJoyUAqhmFJAVDEhGRXDAkkWxxThIREVnCkESyxTlJRERkCUMSydbd84+EEPeckwRmJCIi2WBIItmqnn9Uc7itep4Sh9uIiIghiWSLw21ERGQJQxLJFpcAICIiSxiSSL64BAAREVnAkESyxSUAiIjIEoYkki3OSSIiIksYkki2TC0BcPcQ3N3bDElERPLBkESyVWsJAIOQjpkbbuM6SURE8sGQRLJ199BaVWWVdIzDbURExJBEsnX3cFuVoUZI4nAbEZHsMSSRfN29BECN4TYuAUBERAxJJFt3f9y/5nAblwAgIiKGJJItzkkiIiJLGJJItjgniYiILGFIItm6ewkA/D4liUsAEBERQxLJl6kvs63GniQiImJIItky9T1t1Thxm4iIGJJIturUk8QlAIiIZIMhieSrep0klYmQxE+3ERHJHkMSyZalnqSaQ2yck0REJE8MSSRbZuckKYzDEOckERHJE0MSydddX0tSTaFUGIUho+E4ZiQiItlgSCLZunvF7WoKpeL3Cdp39yqxJ4mISDYYkki2zA23KRS/9yTV3AYYkoiI5IQhiWTL3MTtmsNtdw+9cQkAIiL5YEgi2ar+WhJTw23Vc49qbnM+EhGRvDAkkWzZ2pPEoTYiInlhSCLZsmoJgBrbDElERPLCkETyZcUSAEbbnI9ERCQrDEkkW9YsAXD3NhERyYfdQ1JxcTGmT58Of39/ODs7Y9CgQTh06JB0PD8/HxMmTICPjw9cXFwwbNgwZGdnW2xz5cqVdz66XeOPk5NTrfNOnTqFp59+Gq1bt4ZWq8XAgQNx6dKler9Hapo4J4mIiCyxe0iKiYlBWloaVq1ahePHjyM8PByhoaG4fPkyhBCIiIjA+fPnkZKSgiNHjsDf3x+hoaEoLS212K6bmxvy8vKkPxcvXjQ6fu7cOTzyyCMIDAxEeno6fvrpJ8yePdtkmKKWydZ1khiSiIjkxcGev7ysrAwbN25ESkoKQkJCAABz587F1q1bkZSUhOjoaBw4cABZWVno1asXACApKQne3t5ITk5GTEyM2bYVCgW8vb3NHn/99dcxYsQILFq0SNrXpUuXerozag64BAAREVli156kyspKGAyGWr03zs7OyMjIQHl5OQAYHVcqldBoNMjIyLDYdklJCfz9/eHn54dRo0bhxIkT0rGqqips27YN3bp1w9ChQ9GuXTsEBQXhq6++qr+boyaPw21ERGSJXXuSXF1dERwcjPnz56NHjx7w8vJCcnIy9u/fj4CAAAQGBqJDhw5ITEzE8uXLodVqsXTpUuTm5iIvL89su927d8eKFSvQp08fFBYW4r333sOgQYNw4sQJtG/fHteuXUNJSQneeecdvPXWW3j33Xexc+dOPPvss/juu+/w2GOP1WqzvLxcCm0AUFRUBADQ6/XQ6/X1Wpfq9uq73ZbofmpVZai687+iyviA8vdepprbCqWi2f+d8L1lPdbKeqyVbVgv6zVErWxpSyGk/xrYx7lz5zBp0iTs3bsXKpUK/fr1Q7du3ZCZmYlTp04hMzMTkydPxrFjx6BSqRAaGgqlUgkhBHbs2GHV79Dr9ejRowfGjBmD+fPn48qVK/D19cWYMWOwdu1a6bynn34aWq0WycnJtdqYO3cu5s2bV2v/2rVr4eLiUvcCkN2cm3cOxUeK0SG+Ay59cAn4b1ZSP6CGx0gP5H2eB4c2DmgX0Q5XVl6BylWF3qt62/eiiYjovty6dQtjx45FYWEh3NzcLJ5r154k4M48oD179qC0tBRFRUXQ6XSIiopC586dAQD9+/fH0aNHUVhYiIqKCnh6eiIoKAgDBgyw+neo1Wr88Y9/RE5ODgDAw8MDDg4O6Nmzp9F5PXr0MDuMl5iYiISEBOl1UVER/Pz8EB4efs8i20qv1yMtLQ1hYWFQq9X12nZLcz+1Sv4oGcUoxh/+8Af8ovgF4r8LJzm7OCMwMBB5yIOTsxN69OyBK7gCR40jRowY0RC30Wj43rIea2U91so2rJf1GqJW1SNB1rB7SKqm1Wqh1Wpx8+ZNpKamGk2oBoDWrVsDALKzs3H48GHMnz/f6rYNBgOOHz8u/QfO0dERAwcOxJkzZ4zOO3v2LPz9/U22odFooNFoau1Xq9UN9iZvyLZbmrrUSiHuzDFycHSAQqmAMPw+rOagdqi1rVQqW8zfB99b1mOtrMda2Yb1sl591sqWduweklJTUyGEQPfu3ZGTk4OZM2ciMDAQEydOBACsX78enp6e6NChA44fP474+HhEREQgPDxcaiM6Ohq+vr5YuHAhAODNN9/En/70JwQEBKCgoACLFy/GxYsXjT4NN3PmTERFRSEkJARDhgzBzp07sXXrVqSnpzfq/ZP9VE/cNvoaEnAJACIiusPuIamwsBCJiYnIzc2Fu7s7Ro8ejQULFkhJLy8vDwkJCcjPz4dOp0N0dDRmz55t1MalS5egVP7+CaWbN2/ipZdewtWrV9G2bVv0798f+/btMxpee+aZZ7Bs2TIsXLgQcXFx6N69OzZu3IhHHnmkcW6c7K7mhOyaXznCJQCIiAhoAiEpMjISkZGRZo/HxcUhLi7OYht39/4sXboUS5cuvefvnjRpEiZNmmTVdVLLIy0mWeNj/ne/5hIARETyZfcVt4nshSGJiIgsYUgi2ZJCksI4JBnNUaqxzZBERCQvDEkkX9XztmvOO4LxHCWjbQVDEhGRnDAkkWxxuI2IiCxhSCLZYkgiIiJLGJJItqRv5FEYD6UpFDWWAKixzSUAiIjkhSGJZIs9SUREZAlDEskWQxIREVnCkESyxSUAiIjIEoYkki8uAUBERBYwJJFscbiNiIgsYUgi2WJIIiIiSxiSSLaqQxKXACAiIlMYkki2qtdJYk8SERGZ4mDvCyBjFaUVKMorQsW1ChReLISDmn9FllTqK+tcK0O5AQBDEhERmcb/AjcxZ7eexcYxGwEAJ3HSzlfTfNxPraxeAoCfbiMikhWGpCZGoVLAwckBhioDVEqVvS+nWbifWrl3dYdXHy/0iuqF/X/bD4VSgZ7P9YRfsB8e6P4Aej7XE+2D2sMj0AM9n+9Zz1dORERNGUNSE9Pr+V7oFtEN27dvx4gRI6BWq+19SU2aXq+vl1oNeXMIhrw5xGjftNPTpO3YU7F1bpuIiJonTtwmIiIiMoEhiYiIiMgEhiQiIiIiExiSiIiIiExgSCIiIiIygSGJiIiIyASGJCIiIiITGJKIiIiITGBIIiIiIjKBIYmIiIjIBIYkIiIiIhMYkoiIiIhMYEgiIiIiMoEhiYiIiMgEB3tfQHMlhAAAFBUV1Xvber0et27dQlFREdRqdb2335KwVrZhvazHWlmPtbIN62W9hqhV9X+3q/87bglDUh0VFxcDAPz8/Ox8JURERGSr4uJitG7d2uI5CmFNlKJaqqqqcOXKFbi6ukKhUNRr20VFRfDz88Mvv/wCNze3em27pWGtbMN6WY+1sh5rZRvWy3oNUSshBIqLi+Hj4wOl0vKsI/Yk1ZFSqUT79u0b9He4ubnxH5CVWCvbsF7WY62sx1rZhvWyXn3X6l49SNU4cZuIiIjIBIYkIiIiIhMYkpogjUaDOXPmQKPR2PtSmjzWyjasl/VYK+uxVrZhvaxn71px4jYRERGRCexJIiIiIjKBIYmIiIjIBIYkIiIiIhMYkoiIiIhMYEhqYj766CN07NgRTk5OCAoKwg8//GDvS7K7uXPnQqFQGP0JDAyUjt++fRuxsbF44IEH0KpVK4wePRr5+fl2vOLGtXfvXjz11FPw8fGBQqHAV199ZXRcCIE33ngDOp0Ozs7OCA0NRXZ2ttE5N27cwLhx4+Dm5oY2bdpg8uTJKCkpacS7aBz3qtWECRNqvdeGDRtmdI5carVw4UIMHDgQrq6uaNeuHSIiInDmzBmjc6z5t3fp0iWMHDkSLi4uaNeuHWbOnInKysrGvJVGYU29Bg8eXOv99corrxidI4d6JSUloU+fPtICkcHBwdixY4d0vCm9rxiSmpAvvvgCCQkJmDNnDn788Uf07dsXQ4cOxbVr1+x9aXbXq1cv5OXlSX8yMjKkYzNmzMDWrVuxfv167NmzB1euXMGzzz5rx6ttXKWlpejbty8++ugjk8cXLVqEf/zjH1i2bBkOHjwIrVaLoUOH4vbt29I548aNw4kTJ5CWloavv/4ae/fuxcsvv9xYt9Bo7lUrABg2bJjRey05OdnouFxqtWfPHsTGxuLAgQNIS0uDXq9HeHg4SktLpXPu9W/PYDBg5MiRqKiowL59+/DZZ59h5cqVeOONN+xxSw3KmnoBwEsvvWT0/lq0aJF0TC71at++Pd555x1kZmbi8OHDePzxxzFq1CicOHECQBN7XwlqMh566CERGxsrvTYYDMLHx0csXLjQjldlf3PmzBF9+/Y1eaygoECo1Wqxfv16ad+pU6cEALF///5GusKmA4DYvHmz9Lqqqkp4e3uLxYsXS/sKCgqERqMRycnJQgghTp48KQCIQ4cOSefs2LFDKBQKcfny5Ua79sZ2d62EEGL8+PFi1KhRZn9GrrUSQohr164JAGLPnj1CCOv+7W3fvl0olUpx9epV6ZykpCTh5uYmysvLG/cGGtnd9RJCiMcee0zEx8eb/Rk516tt27bi3//+d5N7X7EnqYmoqKhAZmYmQkNDpX1KpRKhoaHYv3+/Ha+sacjOzoaPjw86d+6McePG4dKlSwCAzMxM6PV6o7oFBgaiQ4cOrBuACxcu4OrVq0b1ad26NYKCgqT67N+/H23atMGAAQOkc0JDQ6FUKnHw4MFGv2Z7S09PR7t27dC9e3dMmTIFv/32m3RMzrUqLCwEALi7uwOw7t/e/v370bt3b3h5eUnnDB06FEVFRVKvQUt1d72qrVmzBh4eHnjwwQeRmJiIW7duScfkWC+DwYB169ahtLQUwcHBTe59xS+4bSKuX78Og8Fg9JcOAF5eXjh9+rSdrqppCAoKwsqVK9G9e3fk5eVh3rx5ePTRR5GVlYWrV6/C0dERbdq0MfoZLy8vXL161T4X3IRU18DU+6r62NWrV9GuXTuj4w4ODnB3d5ddDYcNG4Znn30WnTp1wrlz5/CXv/wFw4cPx/79+6FSqWRbq6qqKkyfPh0PP/wwHnzwQQCw6t/e1atXTb73qo+1VKbqBQBjx46Fv78/fHx88NNPP2HWrFk4c+YMNm3aBEBe9Tp+/DiCg4Nx+/ZttGrVCps3b0bPnj1x9OjRJvW+YkiiJm/48OHSdp8+fRAUFAR/f398+eWXcHZ2tuOVUUvzwgsvSNu9e/dGnz590KVLF6Snp+OJJ56w45XZV2xsLLKysozmApJ55upVc+5a7969odPp8MQTT+DcuXPo0qVLY1+mXXXv3h1Hjx5FYWEhNmzYgPHjx2PPnj32vqxaONzWRHh4eEClUtWawZ+fnw9vb287XVXT1KZNG3Tr1g05OTnw9vZGRUUFCgoKjM5h3e6oroGl95W3t3etDwdUVlbixo0bsq9h586d4eHhgZycHADyrNW0adPw9ddf47vvvkP79u2l/db82/P29jb53qs+1hKZq5cpQUFBAGD0/pJLvRwdHREQEID+/ftj4cKF6Nu3L/7+9783ufcVQ1IT4ejoiP79++Pbb7+V9lVVVeHbb79FcHCwHa+s6SkpKcG5c+eg0+nQv39/qNVqo7qdOXMGly5dYt0AdOrUCd7e3kb1KSoqwsGDB6X6BAcHo6CgAJmZmdI5u3fvRlVVlfQQl6vc3Fz89ttv0Ol0AORVKyEEpk2bhs2bN2P37t3o1KmT0XFr/u0FBwfj+PHjRsEyLS0Nbm5u6NmzZ+PcSCO5V71MOXr0KAAYvb/kUq+7VVVVoby8vOm9r+p1Gjjdl3Xr1gmNRiNWrlwpTp48KV5++WXRpk0boxn8cvS///u/Ij09XVy4cEH85z//EaGhocLDw0Ncu3ZNCCHEK6+8Ijp06CB2794tDh8+LIKDg0VwcLCdr7rxFBcXiyNHjogjR44IAGLJkiXiyJEj4uLFi0IIId555x3Rpk0bkZKSIn766ScxatQo0alTJ1FWVia1MWzYMPHHP/5RHDx4UGRkZIiuXbuKMWPG2OuWGoylWhUXF4tXX31V7N+/X1y4cEF88803ol+/fqJr167i9u3bUhtyqdWUKVNE69atRXp6usjLy5P+3Lp1SzrnXv/2KisrxYMPPijCw8PF0aNHxc6dO4Wnp6dITEy0xy01qHvVKycnR7z55pvi8OHD4sKFCyIlJUV07txZhISESG3IpV6vvfaa2LNnj7hw4YL46aefxGuvvSYUCoXYtWuXEKJpva8YkpqYDz74QHTo0EE4OjqKhx56SBw4cMDel2R3UVFRQqfTCUdHR+Hr6yuioqJETk6OdLysrExMnTpVtG3bVri4uIhnnnlG5OXl2fGKG9d3330nANT6M378eCHEnWUAZs+eLby8vIRGoxFPPPGEOHPmjFEbv/32mxgzZoxo1aqVcHNzExMnThTFxcV2uJuGZalWt27dEuHh4cLT01Oo1Wrh7+8vXnrppVr/J0UutTJVJwDi008/lc6x5t/ezz//LIYPHy6cnZ2Fh4eH+N///V+h1+sb+W4a3r3qdenSJRESEiLc3d2FRqMRAQEBYubMmaKwsNCoHTnUa9KkScLf3184OjoKT09P8cQTT0gBSYim9b5SCCFE/fZNERERETV/nJNEREREZAJDEhEREZEJDElEREREJjAkEREREZnAkERERERkAkMSERERkQkMSUREREQmMCQREd1FoVDgq6++svdlEJGdMSQRUYsyYcIERERE2PsyiKgFYEgiIiIiMoEhiYharMGDByMuLg7/93//B3d3d3h7e2Pu3LlG52RnZyMkJAROTk7o2bMn0tLSarXzyy+/IDIyEm3atIG7uztGjRqFn3/+GQBw+vRpuLi4YO3atdL5X375JZydnXHy5MmGvD0iamAMSUTUon322WfQarU4ePAgFi1ahDfffFMKQlVVVXj22Wfh6OiIgwcPYtmyZZg1a5bRz+v1egwdOhSurq74/vvv8Z///AetWrXCsGHDUFFRgcDAQLz33nuYOnUqLl26hNzcXLzyyit499130bNnT3vcMhHVE37BLRG1KBMmTEBBQQG++uorDB48GAaDAd9//710/KGHHsLjjz+Od955B7t27cLIkSNx8eJF+Pj4AAB27tyJ4cOHY/PmzYiIiMDq1avx1ltv4dSpU1AoFACAiooKtGnTBl999RXCw8MBAE8++SSKiorg6OgIlUqFnTt3SucTUfPkYO8LICJqSH369DF6rdPpcO3aNQDAqVOn4OfnJwUkAAgODjY6/9ixY8jJyYGrq6vR/tu3b+PcuXPS6xUrVqBbt25QKpU4ceIEAxJRC8CQREQtmlqtNnqtUChQVVVl9c+XlJSgf//+WLNmTa1jnp6e0vaxY8dQWloKpVKJvLw86HS6ul80ETUJDElEJFs9evTAL7/8YhRqDhw4YHROv3798MUXX6Bdu3Zwc3Mz2c6NGzcwYcIEvP7668jLy8O4cePw448/wtnZucHvgYgaDiduE5FshYaGolu3bhg/fjyOHTuG77//Hq+//rrROePGjYOHhwdGjRqF77//HhcuXEB6ejri4uKQm5sLAHjllVfg5+eHv/71r1iyZAkMBgNeffVVe9wSEdUjhiQiki2lUonNmzejrKwMDz30EGJiYrBgwQKjc1xcXLB371506NABzz77LHr06IHJkyfj9u3bcHNzw+eff47t27dj1apVcHBwgFarxerVq/Hxxx9jx44ddrozIqoP/HQbERERkQnsSSIiIiIygSGJiIiIyASGJCIiIiITGJKIiIiITGBIIiIiIjKBIYmIiIjIBIYkIiIiIhMYkoiIiIhMYEgiIiIiMoEhiYiIiMgEhiQiIiIiExiSiIiIiEz4//NlgUTSf7QyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "indices = np.arange(len(values))\n",
    "plt.plot(indices, values, color='purple')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Accuracy vs. Indices')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(stumps, num,x_test,y_test ):   \n",
    "    new_stumps=stumps[:num+1]\n",
    "    x_new=x_test.T\n",
    "    acc=[[0,0] for i in range(2) ]\n",
    "    acc={\"-1\":[0,0],\"1\":[0,0]}\n",
    "    for i in range(x_new.shape[0]):\n",
    "        acc[str(y_test[i])][1]+=1\n",
    "\n",
    "        if (prediction(new_stumps, x_new[i])==y_test[i]):\n",
    "            acc[str(y_test[i])][0]+=1\n",
    "\n",
    "\n",
    "    cnt=0\n",
    "    tot=0\n",
    "    for i in acc:\n",
    "        t=acc[i]\n",
    "        tot+=((t[0]/t[1])*100)\n",
    "        print(i,\":\",(t[0]/t[1])*100,\"%\")\n",
    "        cnt+=1\n",
    "\n",
    "    print(\"Total Accuracy\",tot/2,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 : 99.79591836734694 %\n",
      "1 : 99.64757709251101 %\n",
      "Total Accuracy 99.72174772992898 %\n"
     ]
    }
   ],
   "source": [
    "find_accuracy(stumps, max_num_tree,x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stump 1 cut 196.10629969876373  best dim  0 alpha  5.399010758105682\n",
      "Total Accuracy 99.55 %\n"
     ]
    }
   ],
   "source": [
    "values=[]\n",
    "weights=np.array([1.0]*(x_train.shape[1]))\n",
    "num_decesion_tree=1\n",
    "stumps=[]\n",
    "minimumm=1000\n",
    "maximum_accuracy=-1\n",
    "max_num_tree=0\n",
    "for st in range(num_decesion_tree):\n",
    "    total_weight=np.sum(weights)\n",
    "    # print(\"total weight for the \", st ,\"the stump is : \", total_weight)\n",
    "    mini_loss=-1\n",
    "    best_cut=None   \n",
    "    best_left=None\n",
    "    best_right=None\n",
    "    best_dimension=None\n",
    "\n",
    "    for itr in range(5):\n",
    "        #all the values are unique so no need to do np.unique() (maybe beacuse of PCA)\n",
    "        curr=x_train[itr]\n",
    "        sorted_indices = np.argsort(curr)\n",
    "        s_vals =curr[sorted_indices]\n",
    "        s_y_train = y_train[sorted_indices]\n",
    "        # s_vals=np.unique(np.array(sorted(x_train[i])))\n",
    "        cut1=(s_vals[0]+s_vals[1])/2\n",
    "        i_fr_1,i_fr_n1,j_fr_1,j_fr_n1=0,0,0,0\n",
    "        i_fr=np.where(s_vals==s_vals[0])[0]\n",
    "        if (s_y_train[0]==1): i_fr_1=1\n",
    "        else: i_fr_n1=1\n",
    "        Left=Region()\n",
    "        Left.make_region_classify(i_fr_1,i_fr_n1)\n",
    "        \n",
    "        j_fr_1=total_1-i_fr_1\n",
    "        j_fr_n1=total_n1-i_fr_n1\n",
    "        Right=Region()\n",
    "        Right.make_region_classify(j_fr_1, j_fr_n1)\n",
    "\n",
    "        Left.misclassified_weight=0\n",
    "        Left.tot_weight=weights[sorted_indices[0]]\n",
    "        Right.misclassified_weight=0\n",
    "        Right.tot_weight=0\n",
    "        for i in range(1,len(sorted_indices)):\n",
    "            Right.tot_weight+=weights[sorted_indices[i]]\n",
    "            if (y_train[sorted_indices[i]]!=Right.category):\n",
    "                Right.misclassified_weight+=weights[sorted_indices[i]]\n",
    "        new_loss=Left.misclassified_weight+Right.misclassified_weight\n",
    "\n",
    "        if (new_loss<=mini_loss or mini_loss==-1):\n",
    "            mini_loss=new_loss\n",
    "            best_cut=cut1   \n",
    "            best_left=copy.deepcopy(Left)\n",
    "            best_right=copy.deepcopy(Right)\n",
    "            best_dimension=itr\n",
    "    \n",
    "        for j in range(1,len(s_vals)-1):\n",
    "            cut=(s_vals[j]+s_vals[j+1])/2\n",
    "\n",
    "            Right.remove(s_vals[j],sorted_indices[j] , y_train,weights)\n",
    "            Left.add(s_vals[j], sorted_indices[j] , y_train , weights)\n",
    "            new_loss=Left.misclassified_weight+Right.misclassified_weight\n",
    "            if (new_loss<=mini_loss or mini_loss==-1):\n",
    "                mini_loss=new_loss\n",
    "                best_cut=cut\n",
    "                best_left=copy.deepcopy(Left)\n",
    "                best_right=copy.deepcopy(Right)\n",
    "                best_dimension=itr\n",
    "    # print()\n",
    "\n",
    "    loss_ratio=(total_weight-mini_loss)/mini_loss\n",
    "    alpha=np.log((total_weight-mini_loss)/mini_loss)\n",
    "    print(\"stump\",st+1,\"cut\",best_cut, \" best dim \", best_dimension,\"alpha \",alpha)\n",
    "    x_train_T=x_train.T\n",
    "    dim_values = x_train_T[:, best_dimension]\n",
    "\n",
    "    left_indices = np.where(dim_values <= best_cut)[0]\n",
    "\n",
    "    right_indices = np.where(dim_values > best_cut)[0]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    misclass_left_indices = [index for index in left_indices if y_train[index] != best_left.category]\n",
    "    misclass_right_indices = [index for index in right_indices if y_train[index] != best_right.category]\n",
    "\n",
    "\n",
    "    # print(loss_ratio)\n",
    "    for index in misclass_left_indices:\n",
    "        weights[index] =(weights[index]) * loss_ratio\n",
    "        \n",
    "    for index in misclass_right_indices:\n",
    "        weights[index] =(weights[index]) * loss_ratio\n",
    "\n",
    "    tree=Stump(best_left,best_right,best_cut,best_dimension,mini_loss,alpha)\n",
    "    stumps.append(tree)\n",
    "\n",
    "    x_new=x_val.T\n",
    "    acc=[[0,0] for i in range(2) ]\n",
    "    acc={\"-1\":[0,0],\"1\":[0,0]}\n",
    "    for i in range(x_new.shape[0]):\n",
    "        acc[str(y_val[i])][1]+=1\n",
    "\n",
    "        if (prediction(stumps, x_new[i])==y_val[i]):\n",
    "            acc[str(y_val[i])][0]+=1\n",
    "\n",
    "    # cate=[-1,1]\n",
    "    cnt=0\n",
    "    tot=0\n",
    "    for i in acc:\n",
    "        t=acc[i]\n",
    "        tot+=((t[0]/t[1])*100)\n",
    "        # print(cate[cnt],\":\",(t[0]/t[1])*100,\"%\")\n",
    "        cnt+=1\n",
    "    minimumm=min(minimumm,tot/2)\n",
    "    \n",
    "    print(\"Total Accuracy\",tot/2,\"%\")\n",
    "    values.append(tot/2)\n",
    "    if (tot/2>maximum_accuracy):\n",
    "        maximum_accuracy=tot/2\n",
    "        max_num_tree=st\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
