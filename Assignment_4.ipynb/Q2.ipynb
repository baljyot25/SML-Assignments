{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "import math\n",
    "from typing import Counter\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "data = np.load(\"C:\\\\Users\\\\baljyot\\\\Downloads\\\\mnist.npz\")\n",
    "x_train=data['x_train']\n",
    "y_train=data['y_train']\n",
    "x_test=data['x_test']\n",
    "y_test=data['y_test']\n",
    "\n",
    "indices_0 = np.where(y_train == 0)[0]\n",
    "indices_1 = np.where(y_train == 1)[0]\n",
    "\n",
    "val_indices_0 = indices_0[:1000]\n",
    "val_indices_1 = indices_1[:1000]\n",
    "\n",
    "train_indices_0 = indices_0[1000:]\n",
    "train_indices_1 = indices_1[1000:]\n",
    "\n",
    "total_1=len(train_indices_1)\n",
    "total_n1=len(train_indices_0)\n",
    "\n",
    "train_indices = np.concatenate([train_indices_0, train_indices_1])\n",
    "\n",
    "val_indices = np.concatenate([val_indices_0, val_indices_1])\n",
    "\n",
    "np.random.shuffle(train_indices)\n",
    "\n",
    "\n",
    "x_val = np.array(x_train[val_indices])\n",
    "y_val = np.array(y_train[val_indices])\n",
    "x_train = np.array(x_train[train_indices])\n",
    "y_train = np.array(y_train[train_indices])\n",
    "\n",
    "indices = np.where((y_test == 0) | (y_test == 1))[0]\n",
    "x_test =np.array(x_test[indices])\n",
    "y_test = np.array(y_test[indices])\n",
    "\n",
    "\n",
    "flattened=[]\n",
    "for i in range(len(x_train)):\n",
    "    flattened.append(x_train[i].flatten())\n",
    "x_train=np.array(flattened)\n",
    "flattened=[]\n",
    "for i in range(len(x_val)):\n",
    "    flattened.append(x_val[i].flatten())\n",
    "x_val=np.array(flattened)\n",
    "flattened=[]\n",
    "for i in range(len(x_test)):\n",
    "    flattened.append(x_test[i].flatten())\n",
    "x_test=np.array(flattened)\n",
    "\n",
    "\n",
    "x_train=np.transpose(x_train)\n",
    "sums=[]\n",
    "centralisedMean=np.mean(x_train,axis=1)\n",
    "centralisedData=[]\n",
    "for i in range(x_train.shape[0]):\n",
    "    l=[]\n",
    "    for j in range(x_train.shape[1]):\n",
    "        l.append(x_train[i][j]- centralisedMean[i])\n",
    "    centralisedData.append(l)\n",
    "centralisedMean=np.array(np.mean(centralisedData,axis=1))\n",
    "\n",
    "S=np.matmul( centralisedData ,np.transpose(centralisedData))/x_train.shape[1]\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(S)\n",
    "eigenvalues = eigenvalues[::-1]\n",
    "U = np.flip(eigenvectors, axis=1)\n",
    "\n",
    "nUp=U[:,:5]\n",
    "\n",
    "x_proj=np.matmul(nUp.T,x_train-x_train.mean(axis=1)[:,np.newaxis])\n",
    "\n",
    "x_test=np.transpose(x_test)\n",
    "x_proj_test=np.matmul(nUp.T,x_test-x_train.mean(axis=1)[:,np.newaxis])\n",
    "\n",
    "x_val=np.transpose(x_val)\n",
    "x_val_proj=np.matmul(nUp.T,x_val-x_train.mean(axis=1)[:,np.newaxis])\n",
    "\n",
    "\n",
    "x_val=x_val_proj\n",
    "x_test=x_proj_test\n",
    "x_train=x_proj\n",
    "\n",
    "\n",
    "y_train = np.array(y_train).astype(np.int8)\n",
    "y_val = np.array(y_val).astype(np.int8)\n",
    "y_test = np.array(y_test).astype(np.int8)\n",
    "\n",
    "y_train[y_train == 0] = -1\n",
    "y_val[y_val == 0] = -1\n",
    "y_test[y_test == 0] = -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Region:\n",
    "   def __init__(self):\n",
    "      self.one=0\n",
    "      self.n_one=0\n",
    "      self.total_frequency=0\n",
    "      self.mean=0\n",
    "   def make_region_predict(self,one,n_one):\n",
    "      self.one=one\n",
    "      self.n_one=n_one\n",
    "      self.total_frequency=one+n_one\n",
    "\n",
    "      self.mean=(self.one-self.n_one)/self.total_frequency\n",
    "\n",
    "   def calc_ssr(self):\n",
    "      return (self.total_frequency-(((self.one-self.n_one)**2)/self.total_frequency))\n",
    "   \n",
    "   def add(self,val , index, y_train):\n",
    "      if y_train[index]==-1:\n",
    "         self.n_one+=1\n",
    "      else:\n",
    "         self.one+=1\n",
    "      self.total_frequency+=1\n",
    "      self.mean=(self.one-self.n_one)/self.total_frequency\n",
    "\n",
    "\n",
    "   def remove(self,val,index,y_train):\n",
    "      if y_train[index]==-1:\n",
    "         self.n_one-=1\n",
    "      else:\n",
    "         self.one-=1\n",
    "      self.total_frequency-=1\n",
    "      self.mean=(self.one-self.n_one)/self.total_frequency\n",
    "   def print_info(self):\n",
    "      print(\"one \",self.one,end=\"  \")\n",
    "      print(\"n_one \",self.n_one,end=\"  \")\n",
    "      print(\"prediction \", self.mean,end=\"  \")\n",
    "\n",
    "      # print(\"misclassified \",self.misclassified_weight,end=\"  \")\n",
    "      print(\"tot freq \",self.total_frequency,end=\" \")\n",
    "      print(\"ssr \" ,(self.total_frequency-(((self.one-self.n_one)**2)/self.total_frequency)))\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self,left,right,cut,dim):\n",
    "        self.Left=left \n",
    "        self.Right=right \n",
    "        self.cut=cut \n",
    "        self.dim=dim \n",
    "    def predict(self,val):\n",
    "        if (val<=self.cut):\n",
    "            return self.Left.mean\n",
    "        else: \n",
    "            return self.Right.mean\n",
    "        \n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new mse  0.9804435551652176 at the  1 th stump\n",
      "new mse  0.9610833899314902 at the  2 th stump\n",
      "new mse  0.9419195042985942 at the  3 th stump\n",
      "new mse  0.9229518982666453 at the  4 th stump\n",
      "new mse  0.9041805718356574 at the  5 th stump\n",
      "new mse  0.8856055250055865 at the  6 th stump\n",
      "new mse  0.8672267577764128 at the  7 th stump\n",
      "new mse  0.8490442701482528 at the  8 th stump\n",
      "new mse  0.8310580621210383 at the  9 th stump\n",
      "new mse  0.8132681336946734 at the  10 th stump\n",
      "new mse  0.7956744848692922 at the  11 th stump\n",
      "new mse  0.7782771156449023 at the  12 th stump\n",
      "new mse  0.761076026021373 at the  13 th stump\n",
      "new mse  0.7440712159988107 at the  14 th stump\n",
      "new mse  0.7272626855771793 at the  15 th stump\n",
      "new mse  0.7106504347564431 at the  16 th stump\n",
      "new mse  0.6942344635367051 at the  17 th stump\n",
      "new mse  0.6780147719178864 at the  18 th stump\n",
      "new mse  0.6619913598999813 at the  19 th stump\n",
      "new mse  0.6461642274830274 at the  20 th stump\n",
      "new mse  0.6305333746670283 at the  21 th stump\n",
      "new mse  0.6150988014519704 at the  22 th stump\n",
      "new mse  0.5998605078378312 at the  23 th stump\n",
      "new mse  0.584818493824637 at the  24 th stump\n",
      "new mse  0.5699727594123394 at the  25 th stump\n",
      "new mse  0.5553233046010257 at the  26 th stump\n",
      "new mse  0.5408701293906272 at the  27 th stump\n",
      "new mse  0.5266132337811749 at the  28 th stump\n",
      "new mse  0.5125526177726606 at the  29 th stump\n",
      "new mse  0.49868828136510723 at the  30 th stump\n",
      "new mse  0.48502022455843863 at the  31 th stump\n",
      "new mse  0.4715484473527261 at the  32 th stump\n",
      "new mse  0.45827294974795596 at the  33 th stump\n",
      "new mse  0.44519373174412397 at the  34 th stump\n",
      "new mse  0.4323107933412495 at the  35 th stump\n",
      "new mse  0.41962413453928 at the  36 th stump\n",
      "new mse  0.40713375533824836 at the  37 th stump\n",
      "new mse  0.39483965573816127 at the  38 th stump\n",
      "new mse  0.382741835739024 at the  39 th stump\n",
      "new mse  0.37084029534079915 at the  40 th stump\n",
      "new mse  0.35913503454354 at the  41 th stump\n",
      "new mse  0.3476260533472067 at the  42 th stump\n",
      "new mse  0.33631335175179794 at the  43 th stump\n",
      "new mse  0.3251969297573204 at the  44 th stump\n",
      "new mse  0.31427678736380876 at the  45 th stump\n",
      "new mse  0.3035529245712035 at the  46 th stump\n",
      "new mse  0.29302534137954417 at the  47 th stump\n",
      "new mse  0.2826940377888381 at the  48 th stump\n",
      "new mse  0.27255901379905134 at the  49 th stump\n",
      "new mse  0.2626202694102125 at the  50 th stump\n",
      "new mse  0.2528778046222921 at the  51 th stump\n",
      "new mse  0.24333161943532067 at the  52 th stump\n",
      "new mse  0.23398171384930244 at the  53 th stump\n",
      "new mse  0.22482808786420094 at the  54 th stump\n",
      "new mse  0.2158707414800463 at the  55 th stump\n",
      "new mse  0.2071096746968154 at the  56 th stump\n",
      "new mse  0.19854488751453175 at the  57 th stump\n",
      "new mse  0.19017637993317377 at the  58 th stump\n",
      "new mse  0.18200415195277694 at the  59 th stump\n",
      "new mse  0.1740282035732901 at the  60 th stump\n",
      "new mse  0.16624853479475402 at the  61 th stump\n",
      "new mse  0.15866514561715114 at the  62 th stump\n",
      "new mse  0.15127803604048423 at the  63 th stump\n",
      "new mse  0.1440872060647603 at the  64 th stump\n",
      "new mse  0.13709265568996964 at the  65 th stump\n",
      "new mse  0.13029438491611392 at the  66 th stump\n",
      "new mse  0.12369239374319027 at the  67 th stump\n",
      "new mse  0.11728668217121772 at the  68 th stump\n",
      "new mse  0.11107725020016428 at the  69 th stump\n",
      "new mse  0.10506409783005853 at the  70 th stump\n",
      "new mse  0.09924722506088812 at the  71 th stump\n",
      "new mse  0.09362663189265451 at the  72 th stump\n",
      "new mse  0.0882023183253615 at the  73 th stump\n",
      "new mse  0.08297428435899795 at the  74 th stump\n",
      "new mse  0.07794252999357565 at the  75 th stump\n",
      "new mse  0.07310705522908811 at the  76 th stump\n",
      "new mse  0.06846786006553394 at the  77 th stump\n",
      "new mse  0.06402494450292694 at the  78 th stump\n",
      "new mse  0.05977830854124684 at the  79 th stump\n",
      "new mse  0.0557279521805088 at the  80 th stump\n",
      "new mse  0.051873875420706164 at the  81 th stump\n",
      "new mse  0.0482160782618373 at the  82 th stump\n",
      "new mse  0.04475456070391061 at the  83 th stump\n",
      "new mse  0.04148932274691783 at the  84 th stump\n",
      "new mse  0.03842036439085989 at the  85 th stump\n",
      "new mse  0.03554768563574103 at the  86 th stump\n",
      "new mse  0.03287128648155885 at the  87 th stump\n",
      "new mse  0.030391166928315037 at the  88 th stump\n",
      "new mse  0.02810732697600639 at the  89 th stump\n",
      "new mse  0.026019766624634504 at the  90 th stump\n",
      "new mse  0.02412848587419926 at the  91 th stump\n",
      "new mse  0.022433484724701566 at the  92 th stump\n",
      "new mse  0.02093476317614039 at the  93 th stump\n",
      "new mse  0.019632321228516162 at the  94 th stump\n",
      "new mse  0.01852615888182906 at the  95 th stump\n",
      "new mse  0.017616276136077608 at the  96 th stump\n",
      "new mse  0.016902672991263974 at the  97 th stump\n",
      "new mse  0.01638534944738695 at the  98 th stump\n",
      "new mse  0.01606430550444591 at the  99 th stump\n",
      "new mse  0.01593954116244216 at the  100 th stump\n",
      "new mse  0.01601105642137578 at the  101 th stump\n",
      "new mse  0.01593982013959901 at the  102 th stump\n",
      "new mse  0.01600948394717438 at the  103 th stump\n",
      "new mse  0.015940116613102955 at the  104 th stump\n",
      "new mse  0.01600792896932094 at the  105 th stump\n",
      "new mse  0.01605832159872804 at the  106 th stump\n",
      "new mse  0.016007752533287827 at the  107 th stump\n",
      "new mse  0.016026448887069965 at the  108 th stump\n",
      "new mse  0.016007795078229907 at the  109 th stump\n",
      "new mse  0.016026157317136656 at the  110 th stump\n",
      "new mse  0.01600775769268923 at the  111 th stump\n",
      "new mse  0.016025872508137126 at the  112 th stump\n",
      "new mse  0.016007727068083604 at the  113 th stump\n",
      "new mse  0.016025594460072526 at the  114 th stump\n",
      "new mse  0.016007703204412173 at the  115 th stump\n",
      "new mse  0.01602532317294225 at the  116 th stump\n",
      "new mse  0.016007686101675748 at the  117 th stump\n",
      "new mse  0.016025058646746713 at the  118 th stump\n",
      "new mse  0.016007675759873295 at the  119 th stump\n",
      "new mse  0.01602480088148557 at the  120 th stump\n",
      "new mse  0.016007672179005422 at the  121 th stump\n",
      "new mse  0.016024549877159337 at the  122 th stump\n",
      "new mse  0.01600767535907297 at the  123 th stump\n",
      "new mse  0.016024305633766844 at the  124 th stump\n",
      "new mse  0.016007685300073907 at the  125 th stump\n",
      "new mse  0.016024068151309905 at the  126 th stump\n",
      "new mse  0.016007702002010415 at the  127 th stump\n",
      "new mse  0.0160238374297866 at the  128 th stump\n",
      "new mse  0.016007725464880423 at the  129 th stump\n",
      "new mse  0.016023613469198495 at the  130 th stump\n",
      "new mse  0.016007755688685697 at the  131 th stump\n",
      "new mse  0.01602339626954435 at the  132 th stump\n",
      "new mse  0.016007792673425027 at the  133 th stump\n",
      "new mse  0.0160231858308253 at the  134 th stump\n",
      "new mse  0.016007836419099423 at the  135 th stump\n",
      "new mse  0.01602298215304021 at the  136 th stump\n",
      "new mse  0.016007886925707766 at the  137 th stump\n",
      "new mse  0.016022785236190037 at the  138 th stump\n",
      "new mse  0.01600794419325144 at the  139 th stump\n",
      "new mse  0.016022595080274597 at the  140 th stump\n",
      "new mse  0.016008008221729344 at the  141 th stump\n",
      "new mse  0.01602241168529343 at the  142 th stump\n",
      "new mse  0.016008079011141454 at the  143 th stump\n",
      "new mse  0.01602223505124758 at the  144 th stump\n",
      "new mse  0.01600815656148815 at the  145 th stump\n",
      "new mse  0.016022065178134915 at the  146 th stump\n",
      "new mse  0.01600824087276979 at the  147 th stump\n",
      "new mse  0.01602190206595809 at the  148 th stump\n",
      "new mse  0.016008331944985906 at the  149 th stump\n",
      "new mse  0.016021745714714344 at the  150 th stump\n",
      "new mse  0.016008429778136223 at the  151 th stump\n",
      "new mse  0.016021596124406 at the  152 th stump\n",
      "new mse  0.016008534372220842 at the  153 th stump\n",
      "new mse  0.016021453295031476 at the  154 th stump\n",
      "new mse  0.01600864572724037 at the  155 th stump\n",
      "new mse  0.016021317226592207 at the  156 th stump\n",
      "new mse  0.01600876384319476 at the  157 th stump\n",
      "new mse  0.016008628571064852 at the  158 th stump\n",
      "new mse  0.016014529267350232 at the  159 th stump\n",
      "new mse  0.01600508222845969 at the  160 th stump\n",
      "new mse  0.016015770120047353 at the  161 th stump\n",
      "new mse  0.01600175816344064 at the  162 th stump\n",
      "new mse  0.01601723325032958 at the  163 th stump\n",
      "new mse  0.015998656376006448 at the  164 th stump\n",
      "new mse  0.016018918658197275 at the  165 th stump\n",
      "new mse  0.015995776866156296 at the  166 th stump\n",
      "new mse  0.016020826343649895 at the  167 th stump\n",
      "new mse  0.015993119633891956 at the  168 th stump\n",
      "new mse  0.016022956306687753 at the  169 th stump\n",
      "new mse  0.015990684679212094 at the  170 th stump\n",
      "new mse  0.01602530854731077 at the  171 th stump\n",
      "new mse  0.015988472002119065 at the  172 th stump\n",
      "new mse  0.016027883065518823 at the  173 th stump\n",
      "new mse  0.015986481602609424 at the  174 th stump\n",
      "new mse  0.01603067986131216 at the  175 th stump\n",
      "new mse  0.01598471348068664 at the  176 th stump\n",
      "new mse  0.016033698934690692 at the  177 th stump\n",
      "new mse  0.015983167636347016 at the  178 th stump\n",
      "new mse  0.01603694028565418 at the  179 th stump\n",
      "new mse  0.015983130311491938 at the  180 th stump\n",
      "new mse  0.016037139221257175 at the  181 th stump\n",
      "new mse  0.015983093518955 at the  182 th stump\n",
      "new mse  0.016037338689178122 at the  183 th stump\n",
      "new mse  0.015983057258735923 at the  184 th stump\n",
      "new mse  0.01603753868941702 at the  185 th stump\n",
      "new mse  0.015983021530834866 at the  186 th stump\n",
      "new mse  0.016037739221973872 at the  187 th stump\n",
      "new mse  0.015982986335251675 at the  188 th stump\n",
      "new mse  0.0160379402868487 at the  189 th stump\n",
      "new mse  0.015982951671986653 at the  190 th stump\n",
      "new mse  0.016038141884041502 at the  191 th stump\n",
      "new mse  0.015982917541039306 at the  192 th stump\n",
      "new mse  0.01603834401355216 at the  193 th stump\n",
      "new mse  0.015982883942409917 at the  194 th stump\n",
      "new mse  0.016038546675380898 at the  195 th stump\n",
      "new mse  0.015982850876098638 at the  196 th stump\n",
      "new mse  0.016038749869527504 at the  197 th stump\n",
      "new mse  0.015982818342105343 at the  198 th stump\n",
      "new mse  0.01603895359599218 at the  199 th stump\n",
      "new mse  0.015982786340430162 at the  200 th stump\n",
      "new mse  0.016039157854774787 at the  201 th stump\n",
      "new mse  0.015982754871072872 at the  202 th stump\n",
      "new mse  0.01603936264587543 at the  203 th stump\n",
      "new mse  0.015982723934033403 at the  204 th stump\n",
      "new mse  0.01603956796929396 at the  205 th stump\n",
      "new mse  0.01598269352931193 at the  206 th stump\n",
      "new mse  0.01603977382503051 at the  207 th stump\n",
      "new mse  0.01598266365690851 at the  208 th stump\n",
      "new mse  0.01603998021308494 at the  209 th stump\n",
      "new mse  0.015982634316823063 at the  210 th stump\n",
      "new mse  0.016040187133457423 at the  211 th stump\n",
      "new mse  0.015982605509055423 at the  212 th stump\n",
      "new mse  0.016040394586147862 at the  213 th stump\n",
      "new mse  0.0159825772336058 at the  214 th stump\n",
      "new mse  0.016040602571156148 at the  215 th stump\n",
      "new mse  0.01598254949047427 at the  216 th stump\n",
      "new mse  0.016040811088482553 at the  217 th stump\n",
      "new mse  0.015982522279660513 at the  218 th stump\n",
      "new mse  0.016041020138126902 at the  219 th stump\n",
      "new mse  0.015982495601165107 at the  220 th stump\n",
      "new mse  0.016041229720089166 at the  221 th stump\n",
      "new mse  0.015982469454987252 at the  222 th stump\n",
      "new mse  0.01604143983436947 at the  223 th stump\n",
      "new mse  0.015982443841127424 at the  224 th stump\n",
      "new mse  0.016041650480967683 at the  225 th stump\n",
      "new mse  0.015982418759585743 at the  226 th stump\n",
      "new mse  0.016041861659883846 at the  227 th stump\n",
      "new mse  0.015982394210361846 at the  228 th stump\n",
      "new mse  0.016042073371118095 at the  229 th stump\n",
      "new mse  0.015982370193456046 at the  230 th stump\n",
      "new mse  0.016042285614670238 at the  231 th stump\n",
      "new mse  0.015982346708868108 at the  232 th stump\n",
      "new mse  0.01604249839054029 at the  233 th stump\n",
      "new mse  0.015982323756598533 at the  234 th stump\n",
      "new mse  0.016042711698728274 at the  235 th stump\n",
      "new mse  0.01598230133664639 at the  236 th stump\n",
      "new mse  0.01604292553923438 at the  237 th stump\n",
      "new mse  0.01598227944901248 at the  238 th stump\n",
      "new mse  0.01604313991205838 at the  239 th stump\n",
      "new mse  0.015982258093696397 at the  240 th stump\n",
      "new mse  0.016043354817200338 at the  241 th stump\n",
      "new mse  0.015982237270698505 at the  242 th stump\n",
      "new mse  0.01604357025466025 at the  243 th stump\n",
      "new mse  0.015982216980018598 at the  244 th stump\n",
      "new mse  0.016043786224438097 at the  245 th stump\n",
      "new mse  0.015982197221656336 at the  246 th stump\n",
      "new mse  0.016044002726534043 at the  247 th stump\n",
      "new mse  0.015982177995612284 at the  248 th stump\n",
      "new mse  0.01604421976094784 at the  249 th stump\n",
      "new mse  0.015982159301886064 at the  250 th stump\n",
      "new mse  0.016044437327679708 at the  251 th stump\n",
      "new mse  0.015982141140478096 at the  252 th stump\n",
      "new mse  0.016044655426729473 at the  253 th stump\n",
      "new mse  0.01598212351138771 at the  254 th stump\n",
      "new mse  0.016044874058097236 at the  255 th stump\n",
      "new mse  0.015982106414615303 at the  256 th stump\n",
      "new mse  0.01604509322178296 at the  257 th stump\n",
      "new mse  0.015982089850161082 at the  258 th stump\n",
      "new mse  0.016045312917786537 at the  259 th stump\n",
      "new mse  0.01598207381802489 at the  260 th stump\n",
      "new mse  0.016045533146108203 at the  261 th stump\n",
      "new mse  0.01598205831820634 at the  262 th stump\n",
      "new mse  0.01604575390674781 at the  263 th stump\n",
      "new mse  0.015982043350706094 at the  264 th stump\n",
      "new mse  0.016045975199705433 at the  265 th stump\n",
      "new mse  0.015982028915523704 at the  266 th stump\n",
      "new mse  0.016046197024980998 at the  267 th stump\n",
      "new mse  0.01598201501265923 at the  268 th stump\n",
      "new mse  0.016046419382574475 at the  269 th stump\n",
      "new mse  0.01598200164211282 at the  270 th stump\n",
      "new mse  0.016046642272485985 at the  271 th stump\n",
      "new mse  0.015981988803884278 at the  272 th stump\n",
      "new mse  0.016046865694715404 at the  273 th stump\n",
      "new mse  0.015981976497973678 at the  274 th stump\n",
      "new mse  0.016047089649262804 at the  275 th stump\n",
      "new mse  0.015981964724381344 at the  276 th stump\n",
      "new mse  0.016047314136128258 at the  277 th stump\n",
      "new mse  0.01598195348310655 at the  278 th stump\n",
      "new mse  0.015975213690916326 at the  279 th stump\n",
      "new mse  0.01600358465645105 at the  280 th stump\n",
      "new mse  0.015965186524410587 at the  281 th stump\n",
      "new mse  0.016018300440485737 at the  282 th stump\n",
      "new mse  0.015964208037815446 at the  283 th stump\n",
      "new mse  0.016035774355299376 at the  284 th stump\n",
      "new mse  0.015964957848375805 at the  285 th stump\n",
      "new mse  0.015959731831004034 at the  286 th stump\n",
      "new mse  0.015988618952181254 at the  287 th stump\n",
      "new mse  0.01595033749991525 at the  288 th stump\n",
      "new mse  0.016023746656973772 at the  289 th stump\n",
      "new mse  0.015953930858489885 at the  290 th stump\n",
      "new mse  0.015885201537168294 at the  291 th stump\n",
      "new mse  0.01586664107476248 at the  292 th stump\n",
      "new mse  0.015897391748956706 at the  293 th stump\n",
      "new mse  0.01586352891372966 at the  294 th stump\n",
      "new mse  0.01585377140967454 at the  295 th stump\n",
      "new mse  0.015884274616244042 at the  296 th stump\n",
      "new mse  0.01584387030560855 at the  297 th stump\n",
      "new mse  0.015790614208556677 at the  298 th stump\n",
      "new mse  0.015764408980526275 at the  299 th stump\n",
      "new mse  0.01577651209563219 at the  300 th stump\n",
      "Minimum MSE  0.015764408980526275\n",
      "Number of Tree for minimum MSE  299\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#total_1 ,totalt_n1 will change need to take of this\n",
    "values=[]\n",
    "num_decesion_tree=300\n",
    "stumps=[]\n",
    "minimumm=1000\n",
    "maximum_accuracy=-1\n",
    "max_num_tree=0\n",
    "residuals=copy.deepcopy(list(y_train))\n",
    "pred_on_val=[0]*y_val.shape[0]\n",
    "minimum_mse=100\n",
    "min_mse_num_tree=-1\n",
    "trees=[]\n",
    "mse_plot=[]\n",
    "for st in range(num_decesion_tree):\n",
    "    mini_ssr=-1\n",
    "    best_cut=None   \n",
    "    best_left=None\n",
    "    best_right=None\n",
    "    best_dimension=None\n",
    "\n",
    "    for itr in range(5):\n",
    "        #all the values are unique so no need to do np.unique() (maybe beacuse of PCA)\n",
    "        curr=x_train[itr]\n",
    "        sorted_indices = np.argsort(curr)\n",
    "        s_vals =curr[sorted_indices]\n",
    "        s_y_train = y_train[sorted_indices]\n",
    "\n",
    "        cut1=(s_vals[0]+s_vals[1])/2\n",
    "        i_fr_1,i_fr_n1,j_fr_1,j_fr_n1=0,0,0,0\n",
    "        i_fr=np.where(s_vals==s_vals[0])[0]\n",
    "        if (s_y_train[0]==1): i_fr_1=1\n",
    "        else: i_fr_n1=1\n",
    "        Left=Region()\n",
    "        Left.make_region_predict(i_fr_1,i_fr_n1)\n",
    "        \n",
    "        j_fr_1=total_1-i_fr_1\n",
    "        j_fr_n1=total_n1-i_fr_n1\n",
    "        Right=Region()\n",
    "        Right.make_region_predict(j_fr_1, j_fr_n1)\n",
    "\n",
    "        new_ssr=Left.calc_ssr()+Right.calc_ssr()\n",
    "\n",
    "        if (new_ssr<=mini_ssr or mini_ssr==-1):\n",
    "            mini_ssr=new_ssr\n",
    "            best_cut=cut1   \n",
    "            best_left=copy.deepcopy(Left)\n",
    "            best_right=copy.deepcopy(Right)\n",
    "            best_dimension=itr\n",
    "    \n",
    "        for j in range(1,len(s_vals)-1):\n",
    "            cut=(s_vals[j]+s_vals[j+1])/2\n",
    "\n",
    "            Right.remove(s_vals[j],sorted_indices[j] , y_train)\n",
    "            Left.add(s_vals[j], sorted_indices[j] , y_train)\n",
    "            new_ssr=Left.calc_ssr()+Right.calc_ssr()\n",
    "\n",
    "            \n",
    "            # print(\"new ssr \", new_ssr,\" cut \",cut, \" dim \",itr)\n",
    "            # Left.print_info()\n",
    "            # Right.print_info()\n",
    "            \n",
    "\n",
    "            if (new_ssr<=mini_ssr or mini_ssr==-1):\n",
    "                mini_ssr=new_ssr\n",
    "                best_cut=cut\n",
    "                best_left=copy.deepcopy(Left)\n",
    "                best_right=copy.deepcopy(Right)\n",
    "                best_dimension=itr\n",
    "   \n",
    "    # print(\"trees \",st+1)\n",
    "    # print(\"mini ssr \",mini_ssr,\"best cut \",best_cut, \"best dim \",best_dimension)\n",
    "    # best_left.print_info()\n",
    "    # best_right.print_info()\n",
    "\n",
    "    best_tree=Tree(best_left,best_right,best_cut,best_dimension)\n",
    "    total_1=0\n",
    "    total_n1=0\n",
    "    trees.append(best_tree)\n",
    "    for l in range(y_train.shape[0]):\n",
    "        # print(residuals[l],end=\" \")\n",
    "        residuals[l]=residuals[l]-((0.01)*(best_tree.predict(x_train[best_dimension][l])))\n",
    "        \n",
    "        if (residuals[l]<0):\n",
    "            y_train[l]=-1\n",
    "            total_n1+=1\n",
    "        else:\n",
    "            y_train[l]=1\n",
    "            total_1+=1\n",
    "    \n",
    "    new_mse=0\n",
    "    # print(new_mse)\n",
    "    for o in range((y_val.shape[0])):\n",
    "        pred_on_val[o] =pred_on_val[o]+ 0.01 * best_tree.predict(x_val[best_dimension][o])\n",
    "        # print(x)\n",
    "        new_mse=new_mse+(pred_on_val[o]- y_val[o])**2\n",
    "\n",
    "    new_mse=new_mse/y_val.shape[0]\n",
    "    if (new_mse<=minimum_mse):\n",
    "        minimum_mse=new_mse\n",
    "        min_mse_num_tree=st+1\n",
    "\n",
    "\n",
    "    print(\"new mse \", new_mse,\"at the \", st+1,\"th stump\")\n",
    "    mse_plot.append(new_mse)\n",
    "print(\"Minimum MSE \",minimum_mse )\n",
    "print(\"Number of Tree for minimum MSE \",min_mse_num_tree )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "indices = np.arange(len(mse_plot))\n",
    "plt.plot(indices, mse_plot, color='purple')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE vs. Indices')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Test set is  0.015011203546894862\n"
     ]
    }
   ],
   "source": [
    "mset=0\n",
    "for i in range(y_test.shape[0]):\n",
    "    predict=0\n",
    "    for j in range(min_mse_num_tree):\n",
    "        predict=predict+0.01* trees[j].predict(x_test[trees[j].dim][i])\n",
    "    mset=mset+(predict- y_test[i])**2\n",
    "\n",
    "mset=mset/y_test.shape[0]\n",
    "print(\"MSE of Test set is \",mset)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum MSE  0.9804435551652176\n",
      "Number of Tree for minimum MSE  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "import math\n",
    "from typing import Counter\n",
    "import copy\n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "data = np.load(\"C:\\\\Users\\\\baljyot\\\\Downloads\\\\mnist.npz\")\n",
    "x_train=data['x_train']\n",
    "y_train=data['y_train']\n",
    "x_test=data['x_test']\n",
    "y_test=data['y_test']\n",
    "\n",
    "indices_0 = np.where(y_train == 0)[0]\n",
    "indices_1 = np.where(y_train == 1)[0]\n",
    "\n",
    "val_indices_0 = indices_0[:1000]\n",
    "val_indices_1 = indices_1[:1000]\n",
    "\n",
    "train_indices_0 = indices_0[1000:]\n",
    "train_indices_1 = indices_1[1000:]\n",
    "\n",
    "total_1=len(train_indices_1)\n",
    "total_n1=len(train_indices_0)\n",
    "\n",
    "train_indices = np.concatenate([train_indices_0, train_indices_1])\n",
    "\n",
    "val_indices = np.concatenate([val_indices_0, val_indices_1])\n",
    "\n",
    "np.random.shuffle(train_indices)\n",
    "\n",
    "\n",
    "x_val = np.array(x_train[val_indices])\n",
    "y_val = np.array(y_train[val_indices])\n",
    "x_train = np.array(x_train[train_indices])\n",
    "y_train = np.array(y_train[train_indices])\n",
    "\n",
    "indices = np.where((y_test == 0) | (y_test == 1))[0]\n",
    "x_test =np.array(x_test[indices])\n",
    "y_test = np.array(y_test[indices])\n",
    "\n",
    "\n",
    "flattened=[]\n",
    "for i in range(len(x_train)):\n",
    "    flattened.append(x_train[i].flatten())\n",
    "x_train=np.array(flattened)\n",
    "flattened=[]\n",
    "for i in range(len(x_val)):\n",
    "    flattened.append(x_val[i].flatten())\n",
    "x_val=np.array(flattened)\n",
    "flattened=[]\n",
    "for i in range(len(x_test)):\n",
    "    flattened.append(x_test[i].flatten())\n",
    "x_test=np.array(flattened)\n",
    "\n",
    "\n",
    "x_train=np.transpose(x_train)\n",
    "sums=[]\n",
    "centralisedMean=np.mean(x_train,axis=1)\n",
    "centralisedData=[]\n",
    "for i in range(x_train.shape[0]):\n",
    "    l=[]\n",
    "    for j in range(x_train.shape[1]):\n",
    "        l.append(x_train[i][j]- centralisedMean[i])\n",
    "    centralisedData.append(l)\n",
    "centralisedMean=np.array(np.mean(centralisedData,axis=1))\n",
    "\n",
    "S=np.matmul( centralisedData ,np.transpose(centralisedData))/x_train.shape[1]\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(S)\n",
    "eigenvalues = eigenvalues[::-1]\n",
    "U = np.flip(eigenvectors, axis=1)\n",
    "\n",
    "nUp=U[:,:5]\n",
    "\n",
    "x_proj=np.matmul(nUp.T,x_train-x_train.mean(axis=1)[:,np.newaxis])\n",
    "\n",
    "x_test=np.transpose(x_test)\n",
    "x_proj_test=np.matmul(nUp.T,x_test-x_train.mean(axis=1)[:,np.newaxis])\n",
    "\n",
    "x_val=np.transpose(x_val)\n",
    "x_val_proj=np.matmul(nUp.T,x_val-x_train.mean(axis=1)[:,np.newaxis])\n",
    "\n",
    "\n",
    "x_val=x_val_proj\n",
    "x_test=x_proj_test\n",
    "x_train=x_proj\n",
    "\n",
    "\n",
    "y_train = np.array(y_train).astype(np.int8)\n",
    "y_val = np.array(y_val).astype(np.int8)\n",
    "y_test = np.array(y_test).astype(np.int8)\n",
    "\n",
    "y_train[y_train == 0] = -1\n",
    "y_val[y_val == 0] = -1\n",
    "y_test[y_test == 0] = -1\n",
    "\n",
    "\n",
    "#total_1 ,totalt_n1 will change need to take of this\n",
    "values=[]\n",
    "num_decesion_tree=1\n",
    "stumps=[]\n",
    "minimumm=1000\n",
    "maximum_accuracy=-1\n",
    "max_num_tree=0\n",
    "residuals=copy.deepcopy(list(y_train))\n",
    "pred_on_val=[0]*y_val.shape[0]\n",
    "minimum_mse=100\n",
    "min_mse_num_tree=-1\n",
    "trees=[]\n",
    "mse_plot=[]\n",
    "for st in range(num_decesion_tree):\n",
    "    mini_ssr=-1\n",
    "    best_cut=None   \n",
    "    best_left=None\n",
    "    best_right=None\n",
    "    best_dimension=None\n",
    "\n",
    "    for itr in range(5):\n",
    "        #all the values are unique so no need to do np.unique() (maybe beacuse of PCA)\n",
    "        curr=x_train[itr]\n",
    "        sorted_indices = np.argsort(curr)\n",
    "        s_vals =curr[sorted_indices]\n",
    "        s_y_train = y_train[sorted_indices]\n",
    "\n",
    "        cut1=(s_vals[0]+s_vals[1])/2\n",
    "        i_fr_1,i_fr_n1,j_fr_1,j_fr_n1=0,0,0,0\n",
    "        i_fr=np.where(s_vals==s_vals[0])[0]\n",
    "        if (s_y_train[0]==1): i_fr_1=1\n",
    "        else: i_fr_n1=1\n",
    "        Left=Region()\n",
    "        Left.make_region_predict(i_fr_1,i_fr_n1)\n",
    "        \n",
    "        j_fr_1=total_1-i_fr_1\n",
    "        j_fr_n1=total_n1-i_fr_n1\n",
    "        Right=Region()\n",
    "        Right.make_region_predict(j_fr_1, j_fr_n1)\n",
    "\n",
    "        new_ssr=Left.calc_ssr()+Right.calc_ssr()\n",
    "\n",
    "        if (new_ssr<=mini_ssr or mini_ssr==-1):\n",
    "            mini_ssr=new_ssr\n",
    "            best_cut=cut1   \n",
    "            best_left=copy.deepcopy(Left)\n",
    "            best_right=copy.deepcopy(Right)\n",
    "            best_dimension=itr\n",
    "    \n",
    "        for j in range(1,len(s_vals)-1):\n",
    "            cut=(s_vals[j]+s_vals[j+1])/2\n",
    "\n",
    "            Right.remove(s_vals[j],sorted_indices[j] , y_train)\n",
    "            Left.add(s_vals[j], sorted_indices[j] , y_train)\n",
    "            new_ssr=Left.calc_ssr()+Right.calc_ssr()\n",
    "\n",
    "            \n",
    "            # print(\"new ssr \", new_ssr,\" cut \",cut, \" dim \",itr)\n",
    "            # Left.print_info()\n",
    "            # Right.print_info()\n",
    "            \n",
    "\n",
    "            if (new_ssr<=mini_ssr or mini_ssr==-1):\n",
    "                mini_ssr=new_ssr\n",
    "                best_cut=cut\n",
    "                best_left=copy.deepcopy(Left)\n",
    "                best_right=copy.deepcopy(Right)\n",
    "                best_dimension=itr\n",
    "   \n",
    "    # print(\"trees \",st+1)\n",
    "    # print(\"mini ssr \",mini_ssr,\"best cut \",best_cut, \"best dim \",best_dimension)\n",
    "    # best_left.print_info()\n",
    "    # best_right.print_info()\n",
    "\n",
    "    best_tree=Tree(best_left,best_right,best_cut,best_dimension)\n",
    "    total_1=0\n",
    "    total_n1=0\n",
    "    trees.append(best_tree)\n",
    "    for l in range(y_train.shape[0]):\n",
    "        # print(residuals[l],end=\" \")\n",
    "        residuals[l]=residuals[l]-((0.01)*(best_tree.predict(x_train[best_dimension][l])))\n",
    "        \n",
    "        if (residuals[l]<0):\n",
    "            y_train[l]=-1\n",
    "            total_n1+=1\n",
    "        else:\n",
    "            y_train[l]=1\n",
    "            total_1+=1\n",
    "    \n",
    "    new_mse=0\n",
    "    # print(new_mse)\n",
    "    for o in range((y_val.shape[0])):\n",
    "        pred_on_val[o] =pred_on_val[o]+ 0.01 * best_tree.predict(x_val[best_dimension][o])\n",
    "        # print(x)\n",
    "        new_mse=new_mse+(pred_on_val[o]- y_val[o])**2\n",
    "\n",
    "    new_mse=new_mse/y_val.shape[0]\n",
    "    if (new_mse<=minimum_mse):\n",
    "        minimum_mse=new_mse\n",
    "        min_mse_num_tree=st+1\n",
    "\n",
    "\n",
    "    # print(\"new mse \", new_mse)\n",
    "    mse_plot.append(new_mse)\n",
    "print(\"Minimum MSE \",minimum_mse )\n",
    "print(\"Number of Tree for minimum MSE \",min_mse_num_tree )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
